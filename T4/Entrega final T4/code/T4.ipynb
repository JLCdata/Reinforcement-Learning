{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrega final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from buffer import Buffer\n",
    "import gym       \n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartpole_reward(observation_batch, action_batch):\n",
    "    # obs = (x, x', theta, theta')\n",
    "    # rew = cos(theta) - 0.01 * x^2\n",
    "\n",
    "    x=observation_batch[:,0]\n",
    "    theta = observation_batch[:, 2]\n",
    "    reward = np.cos(theta) - 0.01 * x**2\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def pendulum_reward(observation_batch, action_batch):\n",
    "    # obs = (cos(theta), sin(theta), theta')\n",
    "    # rew = - theta^2 - 0.1 * (theta')^2 - 0.001 * a^2\n",
    "    \n",
    "    observation_batch[:,0]=np.clip(observation_batch[:,0],-1,1)\n",
    "    observation_batch[:,1]=np.clip(observation_batch[:,1],-1,1)\n",
    "\n",
    "    theta=np.arctan(observation_batch[:, 1]/observation_batch[:, 0])\n",
    "    theta_dot = observation_batch[:, 2]\n",
    "    reward = -theta**2 - 0.1 * theta_dot**2 - 0.001 * action_batch**2\n",
    "    \n",
    "    return reward\n",
    "\n",
    "reward_function = cartpole_reward if env_name == 'CartPole-v1' else pendulum_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self._fc1 = nn.Sequential(\n",
    "        nn.Linear(dim_states+1, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, dim_states)\n",
    "        )\n",
    "        self.continuous_control=continuous_control\n",
    "       \n",
    "    def forward(self, state, action):\n",
    "\n",
    "        if len(state.shape)>1:\n",
    "\n",
    "            concat_o_a=np.concatenate((state,action.reshape(-1,1)),axis=1)\n",
    "            input=torch.from_numpy(concat_o_a).float()\n",
    "            #print(input)\n",
    "            output=self._fc1(input)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            action=np.array(action if self.continuous_control else [action])\n",
    "            #print(action)\n",
    "            #print(state)\n",
    "            concat_o_a=np.concatenate((state,action))\n",
    "            input=torch.from_numpy(concat_o_a).float()\n",
    "            #print(input)\n",
    "            output=self._fc1(input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2186, 0.0980, 0.1308], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name='Pendulum-v1'\n",
    "#env_name='CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "Model_transitions= Model(dim_states, dim_actions,continuous_control)\n",
    "o_t=env.reset()\n",
    "a_t=env.action_space.sample()\n",
    "Model_transitions(o_t,a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSPlanner:\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control, model, planning_horizon, nb_trajectories, reward_function):\n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "        self._continuous_control = continuous_control\n",
    "\n",
    "        self._model = model\n",
    "\n",
    "        self._planning_horizon = planning_horizon\n",
    "        self._nb_trajectories = nb_trajectories\n",
    "        self._reward_function = reward_function\n",
    "\n",
    "        \n",
    "    def generate_plan(self, observation):\n",
    "        # Generate a sequence of random actions\n",
    "        if self._continuous_control:\n",
    "            random_actions = np.array([[np.array([np.random.random()*4-2]).astype(\"float32\") for i in range(self._planning_horizon)] for j in range(self._nb_trajectories)])\n",
    "            \n",
    "        else:\n",
    "            random_actions = np.array([[np.random.randint(2) for i in range(self._planning_horizon)] for j in range(self._nb_trajectories)])\n",
    "            \n",
    "\n",
    "        # Construct initial observation \n",
    "        o_t = np.repeat(observation,self._nb_trajectories).reshape(-1,self._dim_states)\n",
    "        \n",
    "        rewards = np.zeros((self._nb_trajectories, ))\n",
    "        \n",
    "        for i in range(self._planning_horizon):\n",
    "            #print(rewards.shape)\n",
    "\n",
    "            # Get a_t\n",
    "            a_t=random_actions[:,i].squeeze()\n",
    "\n",
    "            # Predict next observation using the model\n",
    "            o_t1=self._model(o_t,a_t).detach().numpy()\n",
    "\n",
    "            # Compute reward (use reward_function)\n",
    "            #print(rewards)\n",
    "            \n",
    "            rewards=rewards+self._reward_function(o_t,a_t)\n",
    "            \n",
    "            # Update\n",
    "            o_t = o_t1\n",
    "        \n",
    "        # Return the best sequence of actions\n",
    "        index_best_actions=np.argmax(rewards)\n",
    "\n",
    "        return random_actions[index_best_actions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon=30\n",
    "nb_trajectories=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBRLAgent:\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control, model_lr, buffer_size, batch_size, \n",
    "                       planning_horizon, nb_trajectories, reward_function):\n",
    "\n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "\n",
    "        self._continuous_control = continuous_control\n",
    "\n",
    "        self._model_lr = model_lr\n",
    "\n",
    "        self._model = Model(self._dim_states, self._dim_actions, self._continuous_control)\n",
    "\n",
    "        # Adam optimizer\n",
    "        self._model_optimizer = AdamW(self._model.parameters(), lr=self._model_lr)\n",
    "\n",
    "        self._buffer = Buffer(self._dim_states, self._dim_actions, buffer_size, batch_size)\n",
    "        \n",
    "        self._planner = RSPlanner(self._dim_states, self._dim_actions, self._continuous_control, \n",
    "                                  self._model, planning_horizon, nb_trajectories, reward_function)\n",
    "\n",
    "\n",
    "    def select_action(self, observation, random=False):\n",
    "\n",
    "        if random:\n",
    "            # Return random action\n",
    "            if self._continuous_control:\n",
    "                return np.array([np.random.random()*4-2]).astype(\"float32\")\n",
    "            \n",
    "            return np.random.randint(2)\n",
    "\n",
    "        # Generate plan\n",
    "        plan = self._planner.generate_plan(observation)\n",
    "\n",
    "        # Return the first action of the plan\n",
    "        if self._continuous_control:\n",
    "            return plan[0]\n",
    "        \n",
    "        return plan[0]\n",
    "\n",
    "\n",
    "    def store_transition(self, s_t, a_t, s_t1):\n",
    "        self._buffer.store_transition(s_t,a_t,s_t1)\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        \n",
    "        s_t,a_t,s_t1=self._buffer.get_batches()\n",
    "\n",
    "        list_loss=[]\n",
    "        for x,y,z in zip(s_t,a_t,s_t1):\n",
    "            \n",
    "            # Use the batches to train the model\n",
    "            # loss: avg((s_t1 - model(s_t, a_t))^2)\n",
    "            #loss=((self._model(x,y)-torch.tensor(z))**2).mean()\n",
    "\n",
    "            loss=F.mse_loss(self._model(x,y).float(), torch.tensor(z).float())\n",
    "            self._model.zero_grad()\n",
    "            loss.backward()\n",
    "            self._model_optimizer.step()\n",
    "            list_loss.append(loss.item())\n",
    "        \n",
    "        epoch=len(list_loss)\n",
    "        epocas=[i for i in range(epoch)]  # Lista con Ã©pocas hasta el ultimo Check Point para poder graficar\n",
    "\n",
    "        plt.plot(epocas,list_loss) # Plot entrenamiento\n",
    "\n",
    "        plt.legend([\"Loss Entrenamiento\"], loc =\"upper right\")\n",
    "        plt.title('Curvas Loss')\n",
    "        plt.xlabel('# Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# InicializaciÃ³n de memory Buffer\n",
    "max_size=7\n",
    "sample_size=2\n",
    "memory=Buffer(dim_states, dim_actions, max_size, sample_size)\n",
    "\n",
    "# SimulaciÃ³n de 6 transiciones\n",
    "s_t=env.reset()\n",
    "\n",
    "for i in range(6):\n",
    "   \n",
    "    a_t=np.random.randint(2)\n",
    "    a_t= np.array([np.random.random()]).astype(\"float32\")\n",
    "\n",
    "    s_t1, r_t, done_t, _ = env.step(a_t)\n",
    "    #print(s_t1, r_t, done_t)\n",
    "\n",
    "    # Guardar\n",
    "    memory.store_transition(s_t, a_t, s_t1)\n",
    "\n",
    "    s_t = s_t1\n",
    "\n",
    "s_t,a_t,s_t1=memory.get_batches()\n",
    "#Model_transitions(s_t[0],a_t[0])\n",
    "observation_batch=s_t[0]\n",
    "action_batch=a_t[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon=30\n",
    "nb_trajectories=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(dim_states, dim_actions, continuous_control)\n",
    "\n",
    "planner = RSPlanner(dim_states, dim_actions, continuous_control, \n",
    "                                  model, planning_horizon, nb_trajectories, reward_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=planner.generate_plan(o_t)#.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBRLAgent:\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control, model_lr, buffer_size, batch_size, \n",
    "                       planning_horizon, nb_trajectories, reward_function):\n",
    "\n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "\n",
    "        self._continuous_control = continuous_control\n",
    "\n",
    "        self._model_lr = model_lr\n",
    "\n",
    "        self._model = Model(self._dim_states, self._dim_actions, self._continuous_control)\n",
    "\n",
    "        # Adam optimizer\n",
    "        self._model_optimizer = AdamW(self._model.parameters(), lr=self._model_lr)\n",
    "\n",
    "        self._buffer = Buffer(self._dim_states, self._dim_actions, buffer_size, batch_size)\n",
    "        \n",
    "        self._planner = RSPlanner(self._dim_states, self._dim_actions, self._continuous_control, \n",
    "                                  self._model, planning_horizon, nb_trajectories, reward_function)\n",
    "\n",
    "\n",
    "    def select_action(self, observation, random=False):\n",
    "\n",
    "        if random:\n",
    "\n",
    "            if self._continuous_control:\n",
    "    \n",
    "                return np.array([np.random.random()]).astype(\"float32\")\n",
    "\n",
    "            else:\n",
    "                return np.random.randint(2)\n",
    "            \n",
    "\n",
    "        # Generate plan\n",
    "        plan = self._planner.generate_plan(observation)\n",
    "\n",
    "        # Return the first action of the plan\n",
    "        if self._continuous_control:\n",
    "            return plan[0]\n",
    "        \n",
    "        return plan[0]\n",
    "\n",
    "\n",
    "    def store_transition(self, s_t, a_t, s_t1):\n",
    "        self._buffer.store_transition(s_t,a_t,s_t1)\n",
    "        #pass\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        \n",
    "        s_t,a_t,s_t1=self._buffer.get_batches()\n",
    "\n",
    "        list_loss=[]\n",
    "        for x,y,z in zip(s_t,a_t,s_t1):\n",
    "            \n",
    "            # Use the batches to train the model\n",
    "            # loss: avg((s_t1 - model(s_t, a_t))^2)\n",
    "            #loss=((self._model(x,y)-torch.tensor(z))**2).mean()\n",
    "\n",
    "            loss=F.mse_loss(self._model(x,y).float(), torch.tensor(z).float())\n",
    "            self._model.zero_grad()\n",
    "            loss.backward()\n",
    "            self._model_optimizer.step()\n",
    "            list_loss.append(loss.item())\n",
    "        \n",
    "        epoch=len(list_loss)\n",
    "        epocas=[i for i in range(epoch)]  # Lista con Ã©pocas hasta el ultimo Check Point para poder graficar\n",
    "\n",
    "        plt.plot(epocas,list_loss) # Plot entrenamiento\n",
    "\n",
    "        plt.legend([\"Loss Entrenamiento\"], loc =\"upper right\")\n",
    "        plt.title('Curvas Loss')\n",
    "        plt.xlabel('# Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0176033 , -0.00558997,  0.0436074 ,  0.02952303], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_t=env.reset()\n",
    "o_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03268986, -0.00782665,  0.02421126,  0.07885374], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(np.random.randint(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_t1, reward, done, _ = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ti_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
