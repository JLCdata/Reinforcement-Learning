{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from buffer import Buffer\n",
    "import gym       \n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parametrización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "env = gym.make('CartPole-v1')\n",
    "#env = gym.make('CartPole-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self._fc1 = nn.Sequential(\n",
    "        nn.Linear(dim_states+1, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, dim_states)\n",
    "    )\n",
    "       \n",
    "    def forward(self, state, action):\n",
    "\n",
    "        if len(state.shape)>1:\n",
    "\n",
    "            concat_o_a=np.concatenate((state,action.reshape(-1,1)),axis=1)\n",
    "            input=torch.from_numpy(concat_o_a).float()\n",
    "            #print(input)\n",
    "            output=self._fc1(input)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            action=np.array(action if continuous_control else [action])\n",
    "            #print(action)\n",
    "            #print(state)\n",
    "            concat_o_a=np.concatenate((state,action))\n",
    "            input=torch.from_numpy(concat_o_a).float()\n",
    "            #print(input)\n",
    "            output=self._fc1(input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (_fc1): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_transitions= Model(dim_states, dim_actions,continuous_control)\n",
    "Model_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03871325,  0.01166419, -0.02463445,  0.0213343 ], dtype=float32)"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_t=env.reset()\n",
    "o_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(o_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t=env.action_space.sample()\n",
    "a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0029, -0.1297,  0.0632,  0.1675], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_transitions(o_t,a_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Muestreo de experiencias y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(self, dim_states, dim_actions, max_size, sample_size):\n",
    "\n",
    "        assert sample_size < max_size, \"Sample size cannot be greater than buffer size\"\n",
    "        \n",
    "        self._buffer_idx     = 0\n",
    "        self._exps_stored    = 0\n",
    "        self._buffer_size    = max_size\n",
    "        self._sample_size    = sample_size\n",
    "\n",
    "        self._s_t_array      = np.zeros((max_size, dim_states))\n",
    "        self._a_t_array      = np.zeros((max_size))\n",
    "        self._s_t1_array     = np.zeros((max_size, dim_states))\n",
    "\n",
    "\n",
    "    def store_transition(self, s_t, a_t, s_t1):\n",
    "        \n",
    "        # Add transition to the buffer\n",
    "        self._s_t_array[self._buffer_idx]=s_t   \n",
    "        self._a_t_array[self._buffer_idx]=a_t  \n",
    "        self._s_t1_array[self._buffer_idx]=s_t1 \n",
    "\n",
    "        # Aumento de indice y reinicio de indice si superamos capacidad\n",
    "        self._buffer_idx = (self._buffer_idx + 1) % self._buffer_size\n",
    "        self._exps_stored += 1\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def get_batches(self):\n",
    "        \n",
    "        assert self._exps_stored + 1 > self._sample_size, \"Not enough samples has been stored to start sampling\"\n",
    "\n",
    "        # Get all the data contained in the buffer as batches \n",
    "        batches_s_t = [self._s_t_array[i:i+self._sample_size] for i in range(0, len(self._s_t_array), self._sample_size)]\n",
    "        batches_a_t = [self._a_t_array[i:i+self._sample_size] for i in range(0, len(self._a_t_array), self._sample_size)]\n",
    "        batches_s_t1 = [self._s_t1_array[i:i+self._sample_size] for i in range(0, len(self._s_t1_array), self._sample_size)]\n",
    "\n",
    "        return [batches_s_t,batches_a_t,batches_s_t1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = gym.make('Pendulum-v1')\n",
    "dim_states = env.observation_space.shape[0]\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de memory Buffer\n",
    "max_size=7\n",
    "sample_size=2\n",
    "memory=Buffer(dim_states, dim_actions, max_size, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.52470106 -0.85128653  0.13863386] -4.586741365925634 False\n",
      "[-0.54451203 -0.838753   -0.46886647] -4.509774062135058 False\n",
      "[-0.5893075  -0.80790883 -1.0878834 ] -4.629895028682279 False\n",
      "[-0.65552187 -0.7551762  -1.6934413 ] -4.962740140969189 False\n",
      "[-0.7363409 -0.6766107 -2.2554572] -5.511061698994816 False\n",
      "[-0.818928  -0.5738963 -2.6378846] -6.26193082052339 False\n"
     ]
    }
   ],
   "source": [
    "# Simulación de 6 transiciones\n",
    "s_t=env.reset()\n",
    "\n",
    "for i in range(6):\n",
    "   \n",
    "    a_t=np.random.randint(2)\n",
    "    a_t= np.array([np.random.random()]).astype(\"float32\")\n",
    "\n",
    "    s_t1, r_t, done_t, _ = env.step(a_t)\n",
    "    print(s_t1, r_t, done_t)\n",
    "\n",
    "    # Guardar\n",
    "    memory.store_transition(s_t, a_t, s_t1)\n",
    "\n",
    "    s_t = s_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 # 2.3\n",
    "class RSPlanner:\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control, model, planning_horizon, nb_trajectories, reward_function):\n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "        self._continuous_control = continuous_control\n",
    "\n",
    "        self._model = model\n",
    "\n",
    "        self._planning_horizon = planning_horizon\n",
    "        self._nb_trajectories = nb_trajectories\n",
    "        self._reward_function = reward_function\n",
    "\n",
    "        \n",
    "    def generate_plan(self, observation):\n",
    "        # Generate a sequence of random actions\n",
    "        if self._continuous_control:\n",
    "            random_actions = None\n",
    "        else:\n",
    "            random_actions = None\n",
    "        \n",
    "        # Construct initial observation \n",
    "        o_t = None\n",
    "\n",
    "        rewards = torch.zeros((self._nb_trajectories, ))\n",
    "        for i in range(self._planning_horizon):\n",
    "            # Get a_t\n",
    "            if self._continuous_control:\n",
    "                a_t = None\n",
    "            else:\n",
    "                a_t = None\n",
    "\n",
    "            # Predict next observation using the model\n",
    "\n",
    "            # Compute reward (use reward_function)\n",
    "            \n",
    "            o_t = o_t1\n",
    "\n",
    "        # Return the best sequence of actions\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MBRLAgent:\n",
    "\n",
    "    def __init__(self, dim_states, dim_actions, continuous_control, model_lr, buffer_size, batch_size, \n",
    "                       planning_horizon, nb_trajectories, reward_function):\n",
    "\n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "\n",
    "        self._continuous_control = continuous_control\n",
    "\n",
    "        self._model_lr = model_lr\n",
    "\n",
    "        self._model = Model(self._dim_states, self._dim_actions, self._continuous_control)\n",
    "\n",
    "        # Adam optimizer\n",
    "        self._model_optimizer = AdamW(self._model.parameters(), lr=self._model_lr)\n",
    "\n",
    "        self._buffer = Buffer(self._dim_states, self._dim_actions, buffer_size, batch_size)\n",
    "        \n",
    "        self._planner = RSPlanner(self._dim_states, self._dim_actions, self._continuous_control, \n",
    "                                  self._model, planning_horizon, nb_trajectories, reward_function)\n",
    "\n",
    "\n",
    "    def select_action(self, observation, random=False):\n",
    "\n",
    "        if random:\n",
    "            # Return random action\n",
    "            if self._continuous_control:\n",
    "\n",
    "                return np.array([np.random.random()]).astype(\"float32\")\n",
    "\n",
    "            else:\n",
    "                return np.random.randint(2)\n",
    "            \n",
    "            \n",
    "        # Generate plan\n",
    "        plan = None\n",
    "\n",
    "        # Return the first action of the plan\n",
    "        if self._continuous_control:\n",
    "            return None\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "    def store_transition(self, s_t, a_t, s_t1):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        \n",
    "        batches = self._buffer.get_batches()\n",
    "        \n",
    "        for batch in batches:\n",
    "            s_t=batch[0]\n",
    "            a_t=batch[1]\n",
    "            s_t1=batch[2]\n",
    "            pred=self._model\n",
    "            # Use the batches to train the model\n",
    "            # loss: avg((s_t1 - model(s_t, a_t))^2)\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (_fc1): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_transitions= Model(dim_states, dim_actions,continuous_control)\n",
    "Model_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "-------\n",
      "-------\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "optimizer=AdamW(Model_transitions.parameters(), lr=0.001)\n",
    "s_t,a_t,s_t1=memory.get_batches()\n",
    "\n",
    "for x,y,z in zip(s_t,a_t,s_t1):\n",
    "    print(\"-------\")\n",
    "    #print(x)\n",
    "    #print(y)\n",
    "    #print(Model_transitions(x,y))\n",
    "    #loss=((Model_transitions(x,y)-torch.tensor(z))**2).mean()\n",
    "    #print(((Model_transitions(x,y)-torch.tensor(z))**2).mean())\n",
    "    # Backpropagation\n",
    "    Model_transitions.zero_grad()\n",
    "    #print(Model_transitions(x,y).view(-1,1).squeeze())\n",
    "    #print(torch.tensor(z).view(-1,1))\n",
    "    loss=F.mse_loss(Model_transitions(x,y).float(), torch.tensor(z).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(\"-------------\")\n",
    "    #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ti_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
