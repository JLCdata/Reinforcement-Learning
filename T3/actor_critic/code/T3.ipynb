{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jose.luis.cadiz\\Anaconda3\\envs\\Ti_RL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gym\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, dim_states, dim_actions, continuous_control):\n",
    "        super(Policy, self).__init__()\n",
    "        # Capas completamente conectadas con activación ReLU\n",
    "        self.fc1 = nn.Linear(dim_states, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # Capa de salida lineal\n",
    "        self.fc3 = nn.Linear(64, dim_actions)\n",
    "        \n",
    "        if continuous_control:\n",
    "            # Parámetro entrenable para la desviación estándar\n",
    "            self.log_std = nn.Parameter(torch.zeros(1, dim_actions))\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "         # tensor format\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state=state\n",
    "            \n",
    "        else:\n",
    "            state = torch.from_numpy(state).unsqueeze(dim=0).float()\n",
    "\n",
    "\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        \n",
    "        # Si las acciones son continuas, muestrear desde una distribución normal\n",
    "        if hasattr(self, \"log_std\"):\n",
    "            std = torch.exp(self.log_std)\n",
    "            dist = torch.distributions.Normal(logits, std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            # Asegurarse de que las acciones están dentro del rango [-1, 1]\n",
    "            action = torch.tanh(action)\n",
    "            return action\n",
    "        # Si las acciones son discretas, muestrear de una distribución categórica\n",
    "        else:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control):\n",
    "        super(Policy, self).__init__()\n",
    "        # MLP, fully connected layers, ReLU activations, linear ouput activation\n",
    "        # dim_states -> 64 -> 64 -> dim_actions\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim_states, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, dim_actions)\n",
    "        )\n",
    "        \n",
    "        if continuous_control:\n",
    "            # trainable parameter\n",
    "            self.log_std = nn.Parameter(torch.zeros(1, dim_actions))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # tensor format\n",
    "        if isinstance(input, torch.Tensor):\n",
    "            input=input\n",
    "            \n",
    "        else:\n",
    "            input = torch.from_numpy(input).unsqueeze(dim=0).float()\n",
    "            \n",
    "        value = self.layers(input)\n",
    "\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 True\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "dim_states = env.observation_space.shape[0]\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "print(dim_states, dim_actions,continuous_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_policy= Policy(dim_states, dim_actions,continuous_control)\n",
    "RN_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_policy.log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81001085, -0.58641493,  0.8076342 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t=env.reset()\n",
    "s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0350]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action=RN_policy(s_t)\n",
    "action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradients:\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, lr, gamma, \n",
    "                 continuous_control=False, reward_to_go=False, use_baseline=False):\n",
    "        \n",
    "        self._learning_rate = lr\n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "\n",
    "        self._continuous_control = continuous_control\n",
    "        self._use_reward_to_go = reward_to_go\n",
    "        self._use_baseline = use_baseline\n",
    "\n",
    "        self._policy = Policy(self._dim_states, self._dim_actions, self._continuous_control)\n",
    "        # Adam optimizer\n",
    "        self._optimizer = AdamW(self._policy.parameters(), lr=self._learning_rate)\n",
    "\n",
    "        self._select_action = self._select_action_continuous if self._continuous_control else self._select_action_discrete\n",
    "        self._compute_loss = self._compute_loss_continuous if self._continuous_control else self._compute_loss_discrete\n",
    "\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        return self._select_action(observation)\n",
    "        \n",
    "\n",
    "    def _select_action_discrete(self, observation):\n",
    "        # sample from categorical distribution\n",
    "        RN_policy=self._policy \n",
    "        logits=RN_policy(observation)\n",
    "\n",
    "        # Probabilidad de cada acción\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Distribución de probabilidad categorica\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "        # Sample de acción\n",
    "        action = dist.sample()\n",
    "     \n",
    "        return action\n",
    "\n",
    "\n",
    "    def _select_action_continuous(self, observation):\n",
    "        # sample from normal distribution\n",
    "        # use the log std trainable parameter\n",
    "\n",
    "        # RN\n",
    "        RN_policy=self._policy\n",
    "\n",
    "        # Parametro log std de la RN\n",
    "        log_std=RN_policy.log_std\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # Politica dada la observación (Representa el promedio de la distribución normal que muestrea acciones)\n",
    "        policy=RN_policy(observation)\n",
    "        \n",
    "        # Distribución normal de parametros mean y std, esta se utiliza para muestrear acciones de modo de tal de explorar el espacio de acciones\n",
    "        dist = torch.distributions.Normal(policy, std)\n",
    "\n",
    "        # sample de acción\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Asegurarse de que las acciones están dentro del rango [-1, 1]\n",
    "        action = torch.tanh(action)\n",
    "        \n",
    "        return action\n",
    "            \n",
    "\n",
    "    def update(self, observation_batch, action_batch, advantage_batch):\n",
    "        # update the policy here\n",
    "        # you should use self._compute_loss \n",
    "\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def _compute_loss_discrete(self, observation_batch, action_batch, advantage_batch):\n",
    "        # use negative logprobs * advantages\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _compute_loss_continuous(self, observation_batch, action_batch, advantage_batch):\n",
    "        # use negative logprobs * advantages\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def estimate_returns(self, rollouts_rew):\n",
    "        estimated_returns = []\n",
    "        for rollout_rew in rollouts_rew:\n",
    "                \n",
    "            if self._use_reward_to_go:\n",
    "                # only for part 2\n",
    "                estimated_return = None\n",
    "            else:\n",
    "                estimated_return = None\n",
    "            \n",
    "            estimated_returns = np.concatenate([estimated_returns, estimated_return])\n",
    "\n",
    "        if self._use_baseline:\n",
    "            # only for part 2\n",
    "            average_return_baseline = None\n",
    "            # Use the baseline:\n",
    "            #estimated_returns -= average_return_baseline\n",
    "\n",
    "        return np.array(estimated_returns, dtype=np.float32)\n",
    "\n",
    "\n",
    "    # It may be useful to discount the rewards using an auxiliary function [optional]\n",
    "    def _discount_rewards(self, rewards):\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jose.luis.cadiz\\Anaconda3\\envs\\Ti_RL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from policy_gradients import PolicyGradients\n",
    "\n",
    "\n",
    "def perform_single_rollout(env, agent, episode_nb, render=False):\n",
    "\n",
    "    # Modify this function to return a tuple of numpy arrays containing (observations, actions, rewards).\n",
    "    # (np.array(obs), np.array(acs), np.array(rws))\n",
    "    # np.array(obs) -> shape: (time_steps, nb_obs)\n",
    "    # np.array(acs) -> shape: (time_steps, nb_acs) if actions are continuous, (time_steps,) if actions are discrete\n",
    "    # np.array(rws) -> shape: (time_steps,)\n",
    "\n",
    "    ob_t = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    nb_steps = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(1. / 60)\n",
    "\n",
    "        action = agent.select_action(ob_t)\n",
    "\n",
    "        ob_t1, reward, done, _ = env.step(action)\n",
    "\n",
    "        ob_t = np.squeeze(ob_t1) # <-- may not be needed depending on gym version\n",
    "        episode_reward += reward\n",
    "        \n",
    "        nb_steps += 1\n",
    "\n",
    "        if done:\n",
    "            return None\n",
    "\n",
    "\n",
    "def sample_rollouts(env, agent, training_iter, min_batch_steps):\n",
    "\n",
    "    sampled_rollouts = []\n",
    "    total_nb_steps = 0\n",
    "    episode_nb = 0\n",
    "    \n",
    "    while total_nb_steps < min_batch_steps:\n",
    "\n",
    "        episode_nb += 1\n",
    "        render = training_iter%10 == 0 and len(sampled_rollouts) == 0 # Change training_iter%10 to any number you want\n",
    "\n",
    "        # Use perform_single_rollout to get data \n",
    "        # Uncomment once perform_single_rollout works.\n",
    "        # Return sampled_rollouts\n",
    "        \"\"\"\n",
    "        sample_rollout = perform_single_rollout(env, agent, episode_nb, render=render)\n",
    "        total_nb_steps += len(sample_rollout[0])\n",
    "\n",
    "        sampled_rollouts.append(sample_rollout)\n",
    "        \"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_pg_agent(env, agent, training_iterations, min_batch_steps):\n",
    "\n",
    "    tr_iters_vec, avg_reward_vec, std_reward_vec, avg_steps_vec, std_steps_vec = [], [], [], [], []\n",
    "    _, (axes) = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "    \n",
    "    for tr_iter in range(training_iterations):\n",
    "        # Sample rollouts using sample_rollouts\n",
    "        sampled_rollouts = None\n",
    "\n",
    "        # Parse sampled observations, actions and reward into three arrays:\n",
    "        # performed_batch_steps >= min_batch_steps\n",
    "        # sampled_obs: Numpy array, shape: (performed_batch_steps, dim_observations)\n",
    "        sampled_obs = None\n",
    "        # sampled_acs: Numpy array, shape: (performed_batch_steps, dim_actions) if actions are continuous, (performed_batch_steps,) if actions are discrete\n",
    "        sampled_acs = None\n",
    "        # sampled_rew: standard array of length equal to the number of trayectories that were sampled.\n",
    "        # You may change the shape of sampled_rew, but it is useful keeping it as is to estimate returns.\n",
    "        sampled_rew = None\n",
    "\n",
    "        # Return estimation\n",
    "        # estimated_returns: Numpy array, shape: (performed_batch_steps, )\n",
    "        estimated_returns = agent.estimate_returns(sampled_rew)\n",
    "\n",
    "        # performance metrics\n",
    "        update_performance_metrics(tr_iter, sampled_rollouts, axes, tr_iters_vec, avg_reward_vec, std_reward_vec, avg_steps_vec, std_steps_vec)\n",
    "\n",
    "        agent.update(sampled_obs, sampled_acs, estimated_returns)\n",
    "    \n",
    "    save_metrics(tr_iters_vec,avg_reward_vec, std_reward_vec)\n",
    "    \n",
    "\n",
    "def update_performance_metrics(tr_iter, sampled_rollouts, axes, tr_iters_vec, avg_reward_vec, std_reward_vec, avg_steps_vec, std_steps_vec):\n",
    "\n",
    "    raw_returns     = np.array([np.sum(rollout[2]) for rollout in sampled_rollouts])\n",
    "    rollout_steps   = np.array([len(rollout[2]) for rollout in sampled_rollouts])\n",
    "\n",
    "    avg_return = np.average(raw_returns)\n",
    "    max_episode_return = np.max(raw_returns)\n",
    "    min_episode_return = np.min(raw_returns)\n",
    "    std_return = np.std(raw_returns)\n",
    "    avg_steps = np.average(rollout_steps)\n",
    "    std_steps = np.std(rollout_steps)\n",
    "\n",
    "    # logs \n",
    "    print('-' * 32)\n",
    "    print('%20s : %5d'   % ('Training iter'     ,(tr_iter + 1)          ))\n",
    "    print('-' * 32)\n",
    "    print('%20s : %5.3g' % ('Max episode return', max_episode_return    ))\n",
    "    print('%20s : %5.3g' % ('Min episode return', min_episode_return    ))\n",
    "    print('%20s : %5.3g' % ('Return avg'        , avg_return            ))\n",
    "    print('%20s : %5.3g' % ('Return std'        , std_return            ))\n",
    "    print('%20s : %5.3g' % ('Steps avg'         , avg_steps             ))\n",
    "    print('%20s : %5.3g' % ('Steps std'         , std_steps             ))\n",
    "\n",
    "    avg_reward_vec.append(avg_return)\n",
    "    std_reward_vec.append(std_return)\n",
    "\n",
    "    avg_steps_vec.append(avg_steps)\n",
    "    std_steps_vec.append(std_steps)\n",
    "\n",
    "    tr_iters_vec.append(tr_iter)\n",
    "\n",
    "    plot_performance_metrics(axes, \n",
    "                            tr_iters_vec, \n",
    "                            avg_reward_vec, \n",
    "                            std_reward_vec, \n",
    "                            avg_steps_vec,\n",
    "                            std_steps_vec)\n",
    "\n",
    "\n",
    "def plot_performance_metrics(axes, tr_iters_vec, avg_reward_vec, std_reward_vec, avg_steps_vec, std_steps_vec):\n",
    "    ax1, ax2 = axes\n",
    "    \n",
    "    [ax.cla() for ax in axes]\n",
    "    ax1.errorbar(tr_iters_vec, avg_reward_vec, yerr=std_reward_vec, marker='.',color='C0')\n",
    "    ax1.set_ylabel('Avg Reward')\n",
    "    ax2.errorbar(tr_iters_vec, avg_steps_vec, yerr=std_steps_vec, marker='.',color='C1')\n",
    "    ax2.set_ylabel('Avg Steps')\n",
    "\n",
    "    [ax.grid('on') for ax in axes]\n",
    "    [ax.set_xlabel('training iteration') for ax in axes]\n",
    "    plt.pause(0.05)\n",
    "\n",
    "\n",
    "def save_metrics(tr_iters_vec, avg_reward_vec, std_reward_vec):\n",
    "    with open('metrics'+datetime.datetime.now().strftime('%H:%M:%S')+'.csv', 'w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, delimiter='\\t')\n",
    "        csv_writer.writerow(['steps', 'avg_reward', 'std_reward'])\n",
    "        for i in range(len(tr_iters_vec)):\n",
    "            csv_writer.writerow([tr_iters_vec[i], avg_reward_vec[i], std_reward_vec[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03680165,  0.9993226 , -0.41511407], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f0759ee940cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_gradients_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mtraining_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                 min_batch_steps=5000)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-3de21c81ac54>\u001b[0m in \u001b[0;36mtrain_pg_agent\u001b[1;34m(env, agent, training_iterations, min_batch_steps)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;31m# Return estimation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# estimated_returns: Numpy array, shape: (performed_batch_steps, )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mestimated_returns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_returns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_rew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m# performance metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jose.luis.cadiz\\Desktop\\Reinforcement-Learning\\T3\\policy_gradients\\policy_gradients.py\u001b[0m in \u001b[0;36mestimate_returns\u001b[1;34m(self, rollouts_rew)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mestimate_returns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrollouts_rew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mestimated_returns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mrollout_rew\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrollouts_rew\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_reward_to_go\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                            dim_actions=dim_actions, \n",
    "                                            lr=0.005,\n",
    "                                            gamma=0.99,\n",
    "                                            continuous_control=continuous_control,\n",
    "                                            reward_to_go=False,\n",
    "                                            use_baseline=False)\n",
    "\n",
    "train_pg_agent(env=env, \n",
    "                agent=policy_gradients_agent, \n",
    "                training_iterations=1000,\n",
    "                min_batch_steps=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ti_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
