{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gym\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Parametrización de política**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control):\n",
    "        super(Policy, self).__init__()\n",
    "        # MLP, fully connected layers, ReLU activations, linear ouput activation\n",
    "        # dim_states -> 64 -> 64 -> dim_actions\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim_states, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, dim_actions)\n",
    "        )\n",
    "        \n",
    "        if continuous_control:\n",
    "            # trainable parameter\n",
    "            self.log_std = nn.Parameter(torch.zeros(1, dim_actions))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # tensor format\n",
    "        if isinstance(input, torch.Tensor):\n",
    "            input=input\n",
    "            \n",
    "        else:\n",
    "            input = torch.from_numpy(input).unsqueeze(dim=0).float()\n",
    "            \n",
    "        value = self.layers(input)\n",
    "        \n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 True\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "dim_states = env.observation_space.shape[0]\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "print(dim_states, dim_actions,continuous_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.18902148], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_policy= Policy(dim_states, dim_actions,continuous_control)\n",
    "RN_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_policy.log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.94356245,  0.33119458, -0.5466567 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t=env.reset()\n",
    "s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0746]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action=RN_policy(s_t)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradients:\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, lr, gamma, \n",
    "                 continuous_control=False, reward_to_go=False, use_baseline=False):\n",
    "        \n",
    "        self._learning_rate = lr\n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "\n",
    "        self._continuous_control = continuous_control\n",
    "        self._use_reward_to_go = reward_to_go\n",
    "        self._use_baseline = use_baseline\n",
    "\n",
    "        self._policy = Policy(self._dim_states, self._dim_actions, self._continuous_control)\n",
    "        # Adam optimizer\n",
    "        self._optimizer = AdamW(self._policy.parameters(), lr=self._learning_rate)\n",
    "\n",
    "        self._select_action = self._select_action_continuous if self._continuous_control else self._select_action_discrete\n",
    "        self._compute_loss = self._compute_loss_continuous if self._continuous_control else self._compute_loss_discrete\n",
    "\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        return self._select_action(observation)\n",
    "        \n",
    "\n",
    "    def _select_action_discrete(self, observation):\n",
    "        # sample from categorical distribution\n",
    "        RN_policy=self._policy \n",
    "        logits=RN_policy(observation)\n",
    "\n",
    "        # Probabilidad de cada acción\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Distribución de probabilidad categorica\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "        # Sample de acción\n",
    "        action = dist.sample().item()\n",
    "     \n",
    "        return action\n",
    "\n",
    "\n",
    "    def _select_action_continuous(self, observation):\n",
    "        # sample from normal distribution\n",
    "        # use the log std trainable parameter\n",
    "\n",
    "        # RN\n",
    "        RN_policy=self._policy\n",
    "\n",
    "        # Parametro log std de la RN\n",
    "        log_std=RN_policy.log_std\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # Politica dada la observación (Representa el promedio de la distribución normal que muestrea acciones)\n",
    "        policy=RN_policy(observation)\n",
    "        \n",
    "        # Distribución normal de parametros mean y std, esta se utiliza para muestrear acciones de modo de tal de explorar el espacio de acciones\n",
    "        dist = torch.distributions.Normal(policy, std)\n",
    "\n",
    "        # sample de acción\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Asegurarse de que las acciones están dentro del rango [-1, 1]\n",
    "        action = torch.tanh(action)\n",
    "        \n",
    "        return action\n",
    "            \n",
    "\n",
    "    def update(self, observation_batch, action_batch, advantage_batch):\n",
    "        # update the policy here\n",
    "        # you should use self._compute_loss \n",
    "\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def _compute_loss_discrete(self, observation_batch, action_batch, advantage_batch):\n",
    "        # use negative logprobs * advantages\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _compute_loss_continuous(self, observation_batch, action_batch, advantage_batch):\n",
    "        # use negative logprobs * advantages\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def estimate_returns(self, rollouts_rew):\n",
    "        estimated_returns = []\n",
    "        for rollout_rew in rollouts_rew:\n",
    "                \n",
    "            if self._use_reward_to_go:\n",
    "                # only for part 2\n",
    "                estimated_return = None\n",
    "            else:\n",
    "                estimated_return = None\n",
    "            \n",
    "            estimated_returns = np.concatenate([estimated_returns, estimated_return])\n",
    "\n",
    "        if self._use_baseline:\n",
    "            # only for part 2\n",
    "            average_return_baseline = None\n",
    "            # Use the baseline:\n",
    "            #estimated_returns -= average_return_baseline\n",
    "\n",
    "        return np.array(estimated_returns, dtype=np.float32)\n",
    "\n",
    "\n",
    "    # It may be useful to discount the rewards using an auxiliary function [optional]\n",
    "    def _discount_rewards(self, rewards):\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Muestreo de trayectorias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from policy_gradients import PolicyGradients\n",
    "\n",
    "\n",
    "def perform_single_rollout(env, agent, episode_nb, render=False):\n",
    "\n",
    "    # Modify this function to return a tuple of numpy arrays containing (observations, actions, rewards).\n",
    "    # (np.array(obs), np.array(acs), np.array(rws))\n",
    "    # np.array(obs) -> shape: (time_steps, nb_obs)\n",
    "    # np.array(acs) -> shape: (time_steps, nb_acs) if actions are continuous, (time_steps,) if actions are discrete\n",
    "    # np.array(rws) -> shape: (time_steps,)\n",
    "\n",
    "    obs_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    ob_t = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    nb_steps = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(1. / 60)\n",
    "\n",
    "        action = agent.select_action(ob_t)\n",
    "        \n",
    "        ob_t1, reward, done, _ = env.step(action)\n",
    "\n",
    "        obs_list.append(ob_t1)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        ob_t = np.squeeze(ob_t1) # <-- may not be needed depending on gym version\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        nb_steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"Largo del episodio {nb_steps}\")\n",
    "            obs_array = np.array(obs_list)\n",
    "            action_array = np.array(action_list)\n",
    "            reward_array = np.array(reward_list)\n",
    "\n",
    "            return obs_array, action_array, reward_array\n",
    "    #return None\n",
    "\n",
    "def sample_rollouts(env, agent, training_iter, min_batch_steps):\n",
    "\n",
    "    sampled_rollouts = []\n",
    "    total_nb_steps = 0\n",
    "    episode_nb = 0\n",
    "    \n",
    "    while total_nb_steps < min_batch_steps:\n",
    "\n",
    "        episode_nb += 1\n",
    "        #render = training_iter%10 == 0 and len(sampled_rollouts) == 0 # Change training_iter%10 to any number you want\n",
    "        render=False\n",
    "        # Use perform_single_rollout to get data \n",
    "        # Uncomment once perform_single_rollout works.\n",
    "        # Return sampled_rollouts\n",
    "       \n",
    "        sample_rollout = perform_single_rollout(env, agent, episode_nb, render=render)\n",
    "        total_nb_steps += len(sample_rollout[0])\n",
    "\n",
    "        sampled_rollouts.append(sample_rollout)\n",
    "        \n",
    "    return sampled_rollouts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 200\n",
      "(200, 3)\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Rollout\n",
    "x1=perform_single_rollout(env, policy_gradients_agent, 1000, render=False)\n",
    "print(x1[0].shape)\n",
    "print(x1[1].shape)\n",
    "print(x1[2].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El número de filas de las observaciones es igual con el largo del episodio, por lo que se concluye el correcto funcionamiento de la función.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n"
     ]
    }
   ],
   "source": [
    "# Sample rollouts\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n",
      "(5000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "sampled_obs = np.concatenate([x2[i][0] for i in range(len(x2))])\n",
    "sampled_action = np.concatenate([x2[i][1] for i in range(len(x2))])\n",
    "sampled_reward = np.concatenate([x2[i][2] for i in range(len(x2))])\n",
    "print(sampled_obs.shape)\n",
    "print(sampled_action.shape)\n",
    "print(sampled_reward.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El largo del registro de sample rollout es al menos el número de sample mini batch, se concluye que la función funciona.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 20\n",
      "(20, 4)\n",
      "(20,)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "# Rollout\n",
    "x1=perform_single_rollout(env, policy_gradients_agent, 1000, render=False)\n",
    "print(x1[0].shape)\n",
    "print(x1[1].shape)\n",
    "print(x1[2].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El número de filas de las observaciones es igual con el largo del episodio, por lo que se concluye el correcto funcionamiento de la función.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 12\n",
      "Largo del episodio 15\n",
      "Largo del episodio 20\n",
      "Largo del episodio 44\n",
      "Largo del episodio 24\n",
      "Largo del episodio 12\n",
      "Largo del episodio 45\n",
      "Largo del episodio 14\n",
      "Largo del episodio 16\n",
      "Largo del episodio 22\n",
      "Largo del episodio 26\n",
      "Largo del episodio 14\n",
      "Largo del episodio 25\n",
      "Largo del episodio 15\n",
      "Largo del episodio 18\n",
      "Largo del episodio 12\n",
      "Largo del episodio 9\n",
      "Largo del episodio 40\n",
      "Largo del episodio 24\n",
      "Largo del episodio 25\n",
      "Largo del episodio 14\n",
      "Largo del episodio 28\n",
      "Largo del episodio 24\n",
      "Largo del episodio 13\n",
      "Largo del episodio 39\n",
      "Largo del episodio 13\n",
      "Largo del episodio 9\n",
      "Largo del episodio 13\n",
      "Largo del episodio 12\n",
      "Largo del episodio 13\n",
      "Largo del episodio 9\n",
      "Largo del episodio 16\n",
      "Largo del episodio 18\n",
      "Largo del episodio 9\n",
      "Largo del episodio 20\n",
      "Largo del episodio 32\n",
      "Largo del episodio 12\n",
      "Largo del episodio 13\n",
      "Largo del episodio 10\n",
      "Largo del episodio 30\n",
      "Largo del episodio 21\n",
      "Largo del episodio 11\n",
      "Largo del episodio 31\n",
      "Largo del episodio 14\n",
      "Largo del episodio 21\n",
      "Largo del episodio 15\n",
      "Largo del episodio 10\n",
      "Largo del episodio 12\n",
      "Largo del episodio 17\n",
      "Largo del episodio 17\n",
      "Largo del episodio 23\n",
      "Largo del episodio 12\n",
      "Largo del episodio 20\n",
      "Largo del episodio 29\n",
      "Largo del episodio 11\n",
      "Largo del episodio 12\n",
      "Largo del episodio 15\n",
      "Largo del episodio 17\n",
      "Largo del episodio 41\n",
      "Largo del episodio 21\n",
      "Largo del episodio 14\n",
      "Largo del episodio 8\n",
      "Largo del episodio 11\n",
      "Largo del episodio 27\n",
      "Largo del episodio 22\n",
      "Largo del episodio 30\n",
      "Largo del episodio 32\n",
      "Largo del episodio 32\n",
      "Largo del episodio 19\n",
      "Largo del episodio 13\n",
      "Largo del episodio 15\n",
      "Largo del episodio 37\n",
      "Largo del episodio 13\n",
      "Largo del episodio 26\n",
      "Largo del episodio 27\n",
      "Largo del episodio 21\n",
      "Largo del episodio 13\n",
      "Largo del episodio 18\n",
      "Largo del episodio 20\n",
      "Largo del episodio 35\n",
      "Largo del episodio 14\n",
      "Largo del episodio 9\n",
      "Largo del episodio 16\n",
      "Largo del episodio 15\n",
      "Largo del episodio 17\n",
      "Largo del episodio 20\n",
      "Largo del episodio 13\n",
      "Largo del episodio 43\n",
      "Largo del episodio 25\n",
      "Largo del episodio 16\n",
      "Largo del episodio 13\n",
      "Largo del episodio 34\n",
      "Largo del episodio 28\n",
      "Largo del episodio 10\n",
      "Largo del episodio 10\n",
      "Largo del episodio 12\n",
      "Largo del episodio 31\n",
      "Largo del episodio 9\n",
      "Largo del episodio 15\n",
      "Largo del episodio 9\n",
      "Largo del episodio 19\n",
      "Largo del episodio 32\n",
      "Largo del episodio 54\n",
      "Largo del episodio 13\n",
      "Largo del episodio 18\n",
      "Largo del episodio 16\n",
      "Largo del episodio 15\n",
      "Largo del episodio 13\n",
      "Largo del episodio 11\n",
      "Largo del episodio 11\n",
      "Largo del episodio 16\n",
      "Largo del episodio 13\n",
      "Largo del episodio 21\n",
      "Largo del episodio 15\n",
      "Largo del episodio 25\n",
      "Largo del episodio 57\n",
      "Largo del episodio 16\n",
      "Largo del episodio 18\n",
      "Largo del episodio 31\n",
      "Largo del episodio 17\n",
      "Largo del episodio 14\n",
      "Largo del episodio 23\n",
      "Largo del episodio 9\n",
      "Largo del episodio 25\n",
      "Largo del episodio 17\n",
      "Largo del episodio 24\n",
      "Largo del episodio 36\n",
      "Largo del episodio 10\n",
      "Largo del episodio 22\n",
      "Largo del episodio 13\n",
      "Largo del episodio 17\n",
      "Largo del episodio 18\n",
      "Largo del episodio 20\n",
      "Largo del episodio 18\n",
      "Largo del episodio 16\n",
      "Largo del episodio 11\n",
      "Largo del episodio 11\n",
      "Largo del episodio 31\n",
      "Largo del episodio 16\n",
      "Largo del episodio 15\n",
      "Largo del episodio 17\n",
      "Largo del episodio 21\n",
      "Largo del episodio 27\n",
      "Largo del episodio 10\n",
      "Largo del episodio 15\n",
      "Largo del episodio 20\n",
      "Largo del episodio 14\n",
      "Largo del episodio 11\n",
      "Largo del episodio 10\n",
      "Largo del episodio 20\n",
      "Largo del episodio 42\n",
      "Largo del episodio 19\n",
      "Largo del episodio 14\n",
      "Largo del episodio 32\n",
      "Largo del episodio 16\n",
      "Largo del episodio 32\n",
      "Largo del episodio 33\n",
      "Largo del episodio 10\n",
      "Largo del episodio 41\n",
      "Largo del episodio 13\n",
      "Largo del episodio 15\n",
      "Largo del episodio 13\n",
      "Largo del episodio 14\n",
      "Largo del episodio 16\n",
      "Largo del episodio 11\n",
      "Largo del episodio 19\n",
      "Largo del episodio 10\n",
      "Largo del episodio 49\n",
      "Largo del episodio 18\n",
      "Largo del episodio 49\n",
      "Largo del episodio 14\n",
      "Largo del episodio 8\n",
      "Largo del episodio 16\n",
      "Largo del episodio 49\n",
      "Largo del episodio 9\n",
      "Largo del episodio 22\n",
      "Largo del episodio 11\n",
      "Largo del episodio 28\n",
      "Largo del episodio 26\n",
      "Largo del episodio 25\n",
      "Largo del episodio 24\n",
      "Largo del episodio 13\n",
      "Largo del episodio 19\n",
      "Largo del episodio 10\n",
      "Largo del episodio 33\n",
      "Largo del episodio 28\n",
      "Largo del episodio 18\n",
      "Largo del episodio 14\n",
      "Largo del episodio 19\n",
      "Largo del episodio 19\n",
      "Largo del episodio 18\n",
      "Largo del episodio 19\n",
      "Largo del episodio 20\n",
      "Largo del episodio 16\n",
      "Largo del episodio 14\n",
      "Largo del episodio 10\n",
      "Largo del episodio 13\n",
      "Largo del episodio 16\n",
      "Largo del episodio 19\n",
      "Largo del episodio 33\n",
      "Largo del episodio 15\n",
      "Largo del episodio 52\n",
      "Largo del episodio 12\n",
      "Largo del episodio 22\n",
      "Largo del episodio 16\n",
      "Largo del episodio 14\n",
      "Largo del episodio 16\n",
      "Largo del episodio 9\n",
      "Largo del episodio 29\n",
      "Largo del episodio 23\n",
      "Largo del episodio 19\n",
      "Largo del episodio 13\n",
      "Largo del episodio 13\n",
      "Largo del episodio 11\n",
      "Largo del episodio 32\n",
      "Largo del episodio 11\n",
      "Largo del episodio 15\n",
      "Largo del episodio 37\n",
      "Largo del episodio 28\n",
      "Largo del episodio 18\n",
      "Largo del episodio 15\n",
      "Largo del episodio 14\n",
      "Largo del episodio 14\n",
      "Largo del episodio 15\n",
      "Largo del episodio 10\n",
      "Largo del episodio 39\n",
      "Largo del episodio 11\n",
      "Largo del episodio 32\n",
      "Largo del episodio 30\n",
      "Largo del episodio 30\n",
      "Largo del episodio 43\n",
      "Largo del episodio 46\n",
      "Largo del episodio 15\n",
      "Largo del episodio 19\n",
      "Largo del episodio 23\n",
      "Largo del episodio 13\n",
      "Largo del episodio 12\n",
      "Largo del episodio 13\n",
      "Largo del episodio 16\n",
      "Largo del episodio 16\n",
      "Largo del episodio 17\n",
      "Largo del episodio 16\n",
      "Largo del episodio 74\n",
      "Largo del episodio 15\n",
      "Largo del episodio 12\n",
      "Largo del episodio 11\n",
      "Largo del episodio 62\n",
      "Largo del episodio 14\n"
     ]
    }
   ],
   "source": [
    "# Sample rollouts\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5004, 4)\n",
      "(5004,)\n",
      "(5004,)\n"
     ]
    }
   ],
   "source": [
    "sampled_obs = np.concatenate([x2[i][0] for i in range(len(x2))])\n",
    "sampled_action = np.concatenate([x2[i][1] for i in range(len(x2))])\n",
    "sampled_reward = np.concatenate([x2[i][2] for i in range(len(x2))])\n",
    "print(sampled_obs.shape)\n",
    "print(sampled_action.shape)\n",
    "print(sampled_reward.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El largo del registro de sample rollout es al menos el número de sample mini batch, se concluye que la función funciona.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Estimación de retornos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gamma=0.99\n",
    "_use_reward_to_go=False\n",
    "_use_baseline=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([i for i in range(10)])*np.array([_gamma**t for t in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=3\n",
    "y[t:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[t:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(3, 10)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(t,len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.970299  , 0.96059601, 0.95099005, 0.94148015, 0.93206535,\n",
       "       0.92274469, 0.91351725])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([_gamma**t for t in range(t,len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_returns( rollouts_rew):\n",
    "        \n",
    "        estimated_returns = []\n",
    "\n",
    "        for rollout_rew in rollouts_rew:\n",
    "            \n",
    "            # Largo del episodio (largo del reward)\n",
    "            n_steps = len(rollout_rew[2])\n",
    "            \n",
    "            if _use_reward_to_go:\n",
    "                \n",
    "                estimated_return = np.zeros(n_steps)\n",
    "\n",
    "                # Se itera sobre cada interacción agente-ambiente\n",
    "                for t in range(n_steps):\n",
    "\n",
    "                    # rollout_rew[2] es el conjunto de reward asociado al episodio\n",
    "                    rewards_from_t = rollout_rew[2][t:]\n",
    "\n",
    "                    # Vector de gammas elevado a step\n",
    "                    vec_gammas=np.array([_gamma**p for p in range(t,len(rollout_rew[2]))])\n",
    "                  \n",
    "                    estimated_return[t] = np.sum(rewards_from_t*vec_gammas)\n",
    "                    \n",
    "            else:\n",
    "                # Método de recompensa-acumulada o acumulative-reward\n",
    "                estimated_return = np.zeros(n_steps)\n",
    "                \n",
    "                discounted_reward_sum = 0\n",
    "                for t in reversed(range(n_steps)):\n",
    "                    discounted_reward_sum = _gamma * discounted_reward_sum + rollout_rew[2][t]\n",
    "                    estimated_return[t] = discounted_reward_sum\n",
    "            \n",
    "            estimated_returns = np.concatenate([estimated_returns, estimated_return])\n",
    "\n",
    "        if _use_baseline:\n",
    "            \n",
    "            average_return_baseline = np.mean(estimated_returns)\n",
    "            estimated_returns -= average_return_baseline\n",
    "\n",
    "        return np.array(estimated_returns, dtype=np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**validación**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CartPole**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 15\n",
      "Largo del episodio 17\n"
     ]
    }
   ],
   "source": [
    "# Sample rollouts (2 episodios): Ejecutar hasta que se generen solo 2 episodios\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_episodio_1=x2[0][1].shape[0]\n",
    "index_episodio_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_episodio_2=x2[1][1].shape[0]\n",
    "index_episodio_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gamma=0.99\n",
    "_use_reward_to_go=False\n",
    "_use_baseline=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.994164 , 13.125419 , 12.247898 , 11.361513 , 10.466174 ,\n",
       "        9.561792 ,  8.648275 ,  7.7255306,  6.793465 ,  5.851985 ,\n",
       "        4.900995 ,  3.940399 ,  2.9701   ,  1.99     ,  1.       ,\n",
       "       15.705681 , 14.854223 , 13.994164 , 13.125419 , 12.247898 ,\n",
       "       11.361513 , 10.466174 ,  9.561792 ,  8.648275 ,  7.7255306,\n",
       "        6.793465 ,  5.851985 ,  4.900995 ,  3.940399 ,  2.9701   ,\n",
       "        1.99     ,  1.       ], dtype=float32)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=estimate_returns(x2)\n",
    "test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La primera trayectoria tiene 19 steps, por lo que dentro del array test, el rango [0:18] corresponde al retorno esperado desde ese step. Para el episodio 2, corresponde desde el elemento 19 en adelante ([19:]).**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Episodio 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.994164 , 13.125419 , 12.247898 , 11.361513 , 10.466174 ,\n",
       "        9.561792 ,  8.648275 ,  7.7255306,  6.793465 ,  5.851985 ,\n",
       "        4.900995 ,  3.940399 ,  2.9701   ,  1.99     ,  1.       ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0:index_episodio_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.99\n",
      "2.9701\n",
      "3.940399\n",
      "4.90099501\n",
      "5.8519850599\n",
      "6.793465209301\n",
      "7.72553055720799\n",
      "8.64827525163591\n",
      "9.561792499119552\n",
      "10.466174574128356\n",
      "11.361512828387072\n",
      "12.247897700103202\n",
      "13.12541872310217\n",
      "13.994164535871148\n"
     ]
    }
   ],
   "source": [
    "retorno=0\n",
    "for t,reward in enumerate(x2[0][2]):\n",
    "    retorno=retorno+(_gamma**t)*reward\n",
    "    print(retorno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.577705"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retorno=np.sum(test[0:index_episodio_1])\n",
    "retorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6385136"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline=np.mean(test[0:index_episodio_1])\n",
    "baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Episodio 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.705681 , 14.854223 , 13.994164 , 13.125419 , 12.247898 ,\n",
       "       11.361513 , 10.466174 ,  9.561792 ,  8.648275 ,  7.7255306,\n",
       "        6.793465 ,  5.851985 ,  4.900995 ,  3.940399 ,  2.9701   ,\n",
       "        1.99     ,  1.       ], dtype=float32)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[index_episodio_1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.99\n",
      "2.9701\n",
      "3.940399\n",
      "4.90099501\n",
      "5.8519850599\n",
      "6.793465209301\n",
      "7.72553055720799\n",
      "8.64827525163591\n",
      "9.561792499119552\n",
      "10.466174574128356\n",
      "11.361512828387072\n",
      "12.247897700103202\n",
      "13.12541872310217\n",
      "13.994164535871148\n",
      "14.854222890512437\n",
      "15.705680661607312\n"
     ]
    }
   ],
   "source": [
    "retorno=0\n",
    "for t,reward in enumerate(x2[1][2]):\n",
    "    retorno=retorno+(_gamma**t)*reward\n",
    "    print(retorno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145.13762"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retorno=np.sum(test[index_episodio_1:])\n",
    "retorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.537507"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline=np.mean(test[index_episodio_1:])\n",
    "baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Policy gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Reducción de varianza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Evaluación del algoritmo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ti_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
