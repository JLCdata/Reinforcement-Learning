{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gym\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Parametrización de política**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, continuous_control):\n",
    "        super(Policy, self).__init__()\n",
    "        # MLP, fully connected layers, ReLU activations, linear ouput activation\n",
    "        # dim_states -> 64 -> 64 -> dim_actions\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim_states, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, dim_actions)\n",
    "        )\n",
    "        \n",
    "        if continuous_control:\n",
    "            # trainable parameter\n",
    "            self.log_std = nn.Parameter(torch.zeros(1, dim_actions))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # tensor format\n",
    "        if isinstance(input, torch.Tensor):\n",
    "            input=input\n",
    "            \n",
    "        else:\n",
    "            input = torch.from_numpy(input).unsqueeze(dim=0).float()\n",
    "            \n",
    "        value = self.layers(input)\n",
    "        \n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 True\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "dim_states = env.observation_space.shape[0]\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "print(dim_states, dim_actions,continuous_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_policy= Policy(dim_states, dim_actions,continuous_control)\n",
    "RN_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_policy.log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84682214,  0.5318762 , -0.5567537 ], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t=env.reset()\n",
    "s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1577]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action=RN_policy(s_t)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradients:\n",
    "    \n",
    "    def __init__(self, dim_states, dim_actions, lr, gamma, \n",
    "                 continuous_control=False, reward_to_go=False, use_baseline=False):\n",
    "        \n",
    "        self._learning_rate = lr\n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "\n",
    "        self._continuous_control = continuous_control\n",
    "        self._use_reward_to_go = reward_to_go\n",
    "        self._use_baseline = use_baseline\n",
    "\n",
    "        self._policy = Policy(self._dim_states, self._dim_actions, self._continuous_control)\n",
    "        # Adam optimizer\n",
    "        self._optimizer = AdamW(self._policy.parameters(), lr=self._learning_rate)\n",
    "\n",
    "        self._select_action = self._select_action_continuous if self._continuous_control else self._select_action_discrete\n",
    "        self._compute_loss = self._compute_loss_continuous if self._continuous_control else self._compute_loss_discrete\n",
    "\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        return self._select_action(observation)\n",
    "        \n",
    "\n",
    "    def _select_action_discrete(self, observation):\n",
    "        # sample from categorical distribution\n",
    "        RN_policy=self._policy \n",
    "        logits=RN_policy(observation)\n",
    "\n",
    "        # Probabilidad de cada acción\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Distribución de probabilidad categorica\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "        # Sample de acción\n",
    "        action = dist.sample()#.item()\n",
    "     \n",
    "        return action\n",
    "\n",
    "\n",
    "    def _select_action_continuous(self, observation):\n",
    "        # sample from normal distribution\n",
    "        # use the log std trainable parameter\n",
    "\n",
    "        # RN\n",
    "        RN_policy=self._policy\n",
    "\n",
    "        # Parametro log std de la RN\n",
    "        log_std=RN_policy.log_std\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # Politica dada la observación (Representa el promedio de la distribución normal que muestrea acciones)\n",
    "        policy=RN_policy(observation)\n",
    "        \n",
    "        # Distribución normal de parametros mean y std, esta se utiliza para muestrear acciones de modo de tal de explorar el espacio de acciones\n",
    "        dist = torch.distributions.Normal(policy, std)\n",
    "\n",
    "        # sample de acción\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Asegurarse de que las acciones están dentro del rango [-1, 1]\n",
    "        #action = torch.tanh(action)\n",
    "        \n",
    "        return action\n",
    "            \n",
    "\n",
    "    def update(self, observation_batch, action_batch, advantage_batch):\n",
    "        # update the policy here\n",
    "        # you should use self._compute_loss \n",
    "\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def _compute_loss_discrete(self, observation_batch, action_batch, advantage_batch):\n",
    "        # use negative logprobs * advantages\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _compute_loss_continuous(self, observation_batch, action_batch, advantage_batch):\n",
    "        # use negative logprobs * advantages\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def estimate_returns(self, rollouts_rew):\n",
    "        estimated_returns = []\n",
    "        for rollout_rew in rollouts_rew:\n",
    "                \n",
    "            if self._use_reward_to_go:\n",
    "                # only for part 2\n",
    "                estimated_return = None\n",
    "            else:\n",
    "                estimated_return = None\n",
    "            \n",
    "            estimated_returns = np.concatenate([estimated_returns, estimated_return])\n",
    "\n",
    "        if self._use_baseline:\n",
    "            # only for part 2\n",
    "            average_return_baseline = None\n",
    "            # Use the baseline:\n",
    "            #estimated_returns -= average_return_baseline\n",
    "\n",
    "        return np.array(estimated_returns, dtype=np.float32)\n",
    "\n",
    "\n",
    "    # It may be useful to discount the rewards using an auxiliary function [optional]\n",
    "    def _discount_rewards(self, rewards):\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Muestreo de trayectorias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from policy_gradients import PolicyGradients\n",
    "\n",
    "\n",
    "def perform_single_rollout(env, agent, episode_nb, render=False):\n",
    "\n",
    "    # Modify this function to return a tuple of numpy arrays containing (observations, actions, rewards).\n",
    "    # (np.array(obs), np.array(acs), np.array(rws))\n",
    "    # np.array(obs) -> shape: (time_steps, nb_obs)\n",
    "    # np.array(acs) -> shape: (time_steps, nb_acs) if actions are continuous, (time_steps,) if actions are discrete\n",
    "    # np.array(rws) -> shape: (time_steps,)\n",
    "\n",
    "    obs_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    ob_t = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    nb_steps = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(1. / 60)\n",
    "\n",
    "        #action = agent.select_action(ob_t)\n",
    "        \n",
    "        action = agent.select_action(ob_t)\n",
    "    \n",
    "        #print(action)\n",
    "\n",
    "        try:    \n",
    "            ob_t1, reward, done, _ = env.step(action)\n",
    "\n",
    "        except:\n",
    "            ob_t1, reward, done, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "        obs_list.append(ob_t1)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        ob_t = np.squeeze(ob_t1) # <-- may not be needed depending on gym version\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        nb_steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"Largo del episodio {nb_steps}\")\n",
    "            obs_array = np.array(obs_list)\n",
    "            action_array = np.array(action_list)\n",
    "            reward_array = np.array(reward_list)\n",
    "\n",
    "            return obs_array, action_array, reward_array\n",
    "    #return None\n",
    "\n",
    "def sample_rollouts(env, agent, training_iter, min_batch_steps):\n",
    "\n",
    "    sampled_rollouts = []\n",
    "    total_nb_steps = 0\n",
    "    episode_nb = 0\n",
    "    \n",
    "    while total_nb_steps < min_batch_steps:\n",
    "\n",
    "        episode_nb += 1\n",
    "        #render = training_iter%10 == 0 and len(sampled_rollouts) == 0 # Change training_iter%10 to any number you want\n",
    "        render=False\n",
    "        # Use perform_single_rollout to get data \n",
    "        # Uncomment once perform_single_rollout works.\n",
    "        # Return sampled_rollouts\n",
    "       \n",
    "        sample_rollout = perform_single_rollout(env, agent, episode_nb, render=render)\n",
    "        total_nb_steps += len(sample_rollout[0])\n",
    "\n",
    "        sampled_rollouts.append(sample_rollout)\n",
    "        \n",
    "    return sampled_rollouts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 200\n",
      "(200, 3)\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Rollout\n",
    "x1=perform_single_rollout(env, policy_gradients_agent, 1000, render=False)\n",
    "print(x1[0].shape)\n",
    "print(x1[1].shape)\n",
    "print(x1[2].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El número de filas de las observaciones es igual con el largo del episodio, por lo que se concluye el correcto funcionamiento de la función.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "Largo del episodio 200\n"
     ]
    }
   ],
   "source": [
    "# Sample rollouts\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n",
      "(5000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "sampled_obs = np.concatenate([x2[i][0] for i in range(len(x2))])\n",
    "sampled_action = np.concatenate([x2[i][1] for i in range(len(x2))])\n",
    "sampled_reward = np.concatenate([x2[i][2] for i in range(len(x2))])\n",
    "print(sampled_obs.shape)\n",
    "print(sampled_action.shape)\n",
    "print(sampled_reward.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El largo del registro de sample rollout es al menos el número de sample mini batch, se concluye que la función funciona.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 22\n",
      "(22, 4)\n",
      "(22,)\n",
      "(22,)\n"
     ]
    }
   ],
   "source": [
    "# Rollout\n",
    "x1=perform_single_rollout(env, policy_gradients_agent, 1000, render=False)\n",
    "print(x1[0].shape)\n",
    "print(x1[1].shape)\n",
    "print(x1[2].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El número de filas de las observaciones es igual con el largo del episodio, por lo que se concluye el correcto funcionamiento de la función.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 21\n",
      "Largo del episodio 11\n",
      "Largo del episodio 25\n",
      "Largo del episodio 34\n",
      "Largo del episodio 23\n",
      "Largo del episodio 53\n",
      "Largo del episodio 20\n",
      "Largo del episodio 53\n",
      "Largo del episodio 48\n",
      "Largo del episodio 22\n",
      "Largo del episodio 16\n",
      "Largo del episodio 16\n",
      "Largo del episodio 11\n",
      "Largo del episodio 39\n",
      "Largo del episodio 12\n",
      "Largo del episodio 45\n",
      "Largo del episodio 20\n",
      "Largo del episodio 32\n",
      "Largo del episodio 20\n",
      "Largo del episodio 68\n",
      "Largo del episodio 10\n",
      "Largo del episodio 14\n",
      "Largo del episodio 18\n",
      "Largo del episodio 18\n",
      "Largo del episodio 23\n",
      "Largo del episodio 19\n",
      "Largo del episodio 24\n",
      "Largo del episodio 21\n",
      "Largo del episodio 22\n",
      "Largo del episodio 42\n",
      "Largo del episodio 37\n",
      "Largo del episodio 15\n",
      "Largo del episodio 21\n",
      "Largo del episodio 21\n",
      "Largo del episodio 27\n",
      "Largo del episodio 33\n",
      "Largo del episodio 14\n",
      "Largo del episodio 18\n",
      "Largo del episodio 50\n",
      "Largo del episodio 14\n",
      "Largo del episodio 52\n",
      "Largo del episodio 14\n",
      "Largo del episodio 36\n",
      "Largo del episodio 16\n",
      "Largo del episodio 22\n",
      "Largo del episodio 15\n",
      "Largo del episodio 42\n",
      "Largo del episodio 17\n",
      "Largo del episodio 19\n",
      "Largo del episodio 21\n",
      "Largo del episodio 27\n",
      "Largo del episodio 12\n",
      "Largo del episodio 19\n",
      "Largo del episodio 26\n",
      "Largo del episodio 25\n",
      "Largo del episodio 28\n",
      "Largo del episodio 31\n",
      "Largo del episodio 14\n",
      "Largo del episodio 16\n",
      "Largo del episodio 29\n",
      "Largo del episodio 14\n",
      "Largo del episodio 26\n",
      "Largo del episodio 42\n",
      "Largo del episodio 11\n",
      "Largo del episodio 9\n",
      "Largo del episodio 21\n",
      "Largo del episodio 30\n",
      "Largo del episodio 13\n",
      "Largo del episodio 45\n",
      "Largo del episodio 15\n",
      "Largo del episodio 13\n",
      "Largo del episodio 11\n",
      "Largo del episodio 14\n",
      "Largo del episodio 19\n",
      "Largo del episodio 26\n",
      "Largo del episodio 14\n",
      "Largo del episodio 16\n",
      "Largo del episodio 29\n",
      "Largo del episodio 18\n",
      "Largo del episodio 11\n",
      "Largo del episodio 28\n",
      "Largo del episodio 48\n",
      "Largo del episodio 15\n",
      "Largo del episodio 58\n",
      "Largo del episodio 12\n",
      "Largo del episodio 15\n",
      "Largo del episodio 12\n",
      "Largo del episodio 16\n",
      "Largo del episodio 14\n",
      "Largo del episodio 21\n",
      "Largo del episodio 16\n",
      "Largo del episodio 12\n",
      "Largo del episodio 21\n",
      "Largo del episodio 23\n",
      "Largo del episodio 16\n",
      "Largo del episodio 14\n",
      "Largo del episodio 18\n",
      "Largo del episodio 18\n",
      "Largo del episodio 18\n",
      "Largo del episodio 15\n",
      "Largo del episodio 11\n",
      "Largo del episodio 26\n",
      "Largo del episodio 41\n",
      "Largo del episodio 48\n",
      "Largo del episodio 43\n",
      "Largo del episodio 12\n",
      "Largo del episodio 39\n",
      "Largo del episodio 10\n",
      "Largo del episodio 21\n",
      "Largo del episodio 17\n",
      "Largo del episodio 31\n",
      "Largo del episodio 23\n",
      "Largo del episodio 14\n",
      "Largo del episodio 31\n",
      "Largo del episodio 17\n",
      "Largo del episodio 15\n",
      "Largo del episodio 16\n",
      "Largo del episodio 12\n",
      "Largo del episodio 20\n",
      "Largo del episodio 15\n",
      "Largo del episodio 20\n",
      "Largo del episodio 13\n",
      "Largo del episodio 24\n",
      "Largo del episodio 24\n",
      "Largo del episodio 64\n",
      "Largo del episodio 20\n",
      "Largo del episodio 46\n",
      "Largo del episodio 21\n",
      "Largo del episodio 22\n",
      "Largo del episodio 22\n",
      "Largo del episodio 11\n",
      "Largo del episodio 22\n",
      "Largo del episodio 33\n",
      "Largo del episodio 13\n",
      "Largo del episodio 33\n",
      "Largo del episodio 29\n",
      "Largo del episodio 14\n",
      "Largo del episodio 20\n",
      "Largo del episodio 20\n",
      "Largo del episodio 13\n",
      "Largo del episodio 13\n",
      "Largo del episodio 25\n",
      "Largo del episodio 25\n",
      "Largo del episodio 14\n",
      "Largo del episodio 13\n",
      "Largo del episodio 14\n",
      "Largo del episodio 14\n",
      "Largo del episodio 15\n",
      "Largo del episodio 34\n",
      "Largo del episodio 105\n",
      "Largo del episodio 62\n",
      "Largo del episodio 11\n",
      "Largo del episodio 32\n",
      "Largo del episodio 18\n",
      "Largo del episodio 13\n",
      "Largo del episodio 10\n",
      "Largo del episodio 11\n",
      "Largo del episodio 45\n",
      "Largo del episodio 24\n",
      "Largo del episodio 20\n",
      "Largo del episodio 18\n",
      "Largo del episodio 12\n",
      "Largo del episodio 14\n",
      "Largo del episodio 29\n",
      "Largo del episodio 15\n",
      "Largo del episodio 14\n",
      "Largo del episodio 17\n",
      "Largo del episodio 23\n",
      "Largo del episodio 11\n",
      "Largo del episodio 32\n",
      "Largo del episodio 16\n",
      "Largo del episodio 13\n",
      "Largo del episodio 27\n",
      "Largo del episodio 39\n",
      "Largo del episodio 17\n",
      "Largo del episodio 10\n",
      "Largo del episodio 23\n",
      "Largo del episodio 22\n",
      "Largo del episodio 15\n",
      "Largo del episodio 40\n",
      "Largo del episodio 39\n",
      "Largo del episodio 11\n",
      "Largo del episodio 12\n",
      "Largo del episodio 23\n",
      "Largo del episodio 27\n",
      "Largo del episodio 30\n",
      "Largo del episodio 29\n",
      "Largo del episodio 43\n",
      "Largo del episodio 27\n",
      "Largo del episodio 22\n",
      "Largo del episodio 10\n",
      "Largo del episodio 17\n",
      "Largo del episodio 22\n",
      "Largo del episodio 17\n",
      "Largo del episodio 15\n",
      "Largo del episodio 34\n",
      "Largo del episodio 21\n",
      "Largo del episodio 22\n",
      "Largo del episodio 11\n",
      "Largo del episodio 11\n",
      "Largo del episodio 29\n",
      "Largo del episodio 34\n",
      "Largo del episodio 21\n",
      "Largo del episodio 19\n",
      "Largo del episodio 26\n",
      "Largo del episodio 39\n",
      "Largo del episodio 14\n",
      "Largo del episodio 21\n",
      "Largo del episodio 26\n",
      "Largo del episodio 11\n",
      "Largo del episodio 12\n",
      "Largo del episodio 32\n",
      "Largo del episodio 19\n",
      "Largo del episodio 19\n",
      "Largo del episodio 43\n"
     ]
    }
   ],
   "source": [
    "# Sample rollouts\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5017, 4)\n",
      "(5017,)\n",
      "(5017,)\n"
     ]
    }
   ],
   "source": [
    "sampled_obs = np.concatenate([x2[i][0] for i in range(len(x2))])\n",
    "sampled_action = np.concatenate([x2[i][1] for i in range(len(x2))])\n",
    "sampled_reward = np.concatenate([x2[i][2] for i in range(len(x2))])\n",
    "print(sampled_obs.shape)\n",
    "print(sampled_action.shape)\n",
    "print(sampled_reward.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El largo del registro de sample rollout es al menos el número de sample mini batch, se concluye que la función funciona.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Estimación de retornos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_returns( rollouts_rew):\n",
    "        estimated_returns = []\n",
    "        for rollout_rew in rollouts_rew:\n",
    "\n",
    "            # Largo del episodio (largo del reward)\n",
    "            n_steps = len(rollout_rew)\n",
    "            estimated_return = np.zeros(n_steps)\n",
    "\n",
    "            if _use_reward_to_go:\n",
    "            \n",
    "                estimated_return = None\n",
    "            else:\n",
    "                estimated_return = np.zeros(n_steps)\n",
    "\n",
    "                vec_gammas=np.array([_gamma**j for j in range(n_steps)])\n",
    "\n",
    "                sum_descount=np.sum(vec_gammas*rollout_rew)\n",
    "\n",
    "                for t in range(n_steps):\n",
    "                    \n",
    "                    estimated_return[t] = sum_descount\n",
    "                    \n",
    "             \n",
    "            estimated_returns = np.concatenate([estimated_returns, estimated_return])\n",
    "\n",
    "        if _use_baseline:\n",
    "            pass\n",
    "            #average_return_baseline = np.mean(estimated_returns)\n",
    "            #estimated_returns -= average_return_baseline\n",
    "\n",
    "        return np.array(estimated_returns, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gamma=0.99\n",
    "_use_reward_to_go=False\n",
    "_use_baseline=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 15\n",
      "Largo del episodio 32\n",
      "\n",
      "Vector de retorno\n",
      "[13.994164 13.994164 13.994164 13.994164 13.994164 13.994164 13.994164\n",
      " 13.994164 13.994164 13.994164 13.994164 13.994164 13.994164 13.994164\n",
      " 13.994164 27.501966 27.501966 27.501966 27.501966 27.501966 27.501966\n",
      " 27.501966 27.501966 27.501966 27.501966 27.501966 27.501966 27.501966\n",
      " 27.501966 27.501966 27.501966 27.501966 27.501966 27.501966 27.501966\n",
      " 27.501966 27.501966 27.501966 27.501966 27.501966 27.501966 27.501966\n",
      " 27.501966 27.501966 27.501966 27.501966 27.501966]\n",
      "\n",
      "Retorno Ep 1\n",
      "13.994164535871148\n",
      "\n",
      "Retorno Ep 2\n",
      "27.501966404214624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)\n",
    "\n",
    "# Sample rollouts (2 episodios): Ejecutar hasta que se generen solo 2 episodios!!\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 22)\n",
    "sampled_rew = [x2[i][2] for i in range(len(x2))]\n",
    "\n",
    "print(\"\")\n",
    "print(\"Vector de retorno\")\n",
    "print(estimate_returns(sampled_rew))\n",
    "print(\"\")\n",
    "retorno=0\n",
    "for t,reward in enumerate(x2[0][2]):\n",
    "    retorno=retorno+(_gamma**t)*reward\n",
    "print(\"Retorno Ep 1\")\n",
    "print(retorno)\n",
    "print(\"\")\n",
    "retorno=0\n",
    "for t,reward in enumerate(x2[1][2]):\n",
    "    retorno=retorno+(_gamma**t)*reward\n",
    "print(\"Retorno Ep 2\")\n",
    "print(retorno)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 200\n",
      "Largo del episodio 200\n",
      "\n",
      "Muestra vector de retorno ep 1\n",
      "-758.4204\n",
      "\n",
      "\n",
      "Muestra vector de retorno ep 2\n",
      "-679.38983\n",
      "\n",
      "Retorno Ep 1\n",
      "-758.4204328420228\n",
      "\n",
      "Retorno Ep 2\n",
      "-679.3898158058794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('Pendulum-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)\n",
    "\n",
    "# Sample rollouts (2 episodios): Ejecutar hasta que se generen solo 2 episodios!!\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 220)\n",
    "sampled_rew = [x2[i][2] for i in range(len(x2))]\n",
    "index_episodio_1=x2[0][1].shape[0]\n",
    "\n",
    "print(\"\")\n",
    "print(\"Muestra vector de retorno ep 1\")\n",
    "print(estimate_returns(sampled_rew)[index_episodio_1-1])\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Muestra vector de retorno ep 2\")\n",
    "print(estimate_returns(sampled_rew)[index_episodio_1+1])\n",
    "print(\"\")\n",
    "\n",
    "retorno=0\n",
    "for t,reward in enumerate(x2[0][2]):\n",
    "    retorno=retorno+(_gamma**t)*reward\n",
    "print(\"Retorno Ep 1\")\n",
    "print(retorno)\n",
    "print(\"\")\n",
    "\n",
    "retorno=0\n",
    "for t,reward in enumerate(x2[1][2]):\n",
    "    retorno=retorno+(_gamma**t)*reward\n",
    "print(\"Retorno Ep 2\")\n",
    "print(retorno)\n",
    "print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se observa que que para ambos ambientes y en cada episodio, los retornos coinciden los calculos obtenidos.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Policy gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "#env=gym.make('Pendulum-v1')\n",
    "env.reset()\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 11\n",
      "Largo del episodio 13\n",
      "Largo del episodio 19\n",
      "Largo del episodio 16\n",
      "Largo del episodio 15\n",
      "Largo del episodio 10\n",
      "Largo del episodio 27\n",
      "Largo del episodio 61\n",
      "Largo del episodio 13\n",
      "Largo del episodio 56\n"
     ]
    }
   ],
   "source": [
    "# Sample rollouts (2 episodios): Ejecutar hasta que se generen solo 2 episodios!!\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241, 4)\n",
      "(241,)\n",
      "(241,)\n"
     ]
    }
   ],
   "source": [
    "sampled_obs = np.concatenate([x2[i][0] for i in range(len(x2))])\n",
    "sampled_action = np.concatenate([x2[i][1] for i in range(len(x2))])\n",
    "sampled_rew = [x2[i][2] for i in range(len(x2))]\n",
    "print(sampled_obs.shape)\n",
    "print(sampled_action.shape)\n",
    "print(sampled_reward.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.1778e-03,  1.6743e-02],\n",
       "         [-3.7328e-03,  1.2797e-02],\n",
       "         [-5.6984e-03,  1.9901e-02],\n",
       "         [ 5.5989e-04,  2.7123e-02],\n",
       "         [-6.2712e-03,  2.0900e-02],\n",
       "         [-9.2645e-04,  2.7185e-02],\n",
       "         [ 1.4100e-02,  2.9771e-02],\n",
       "         [ 2.9394e-02,  3.2230e-02],\n",
       "         [ 4.5814e-02,  3.3812e-02],\n",
       "         [ 6.3555e-02,  3.6900e-02],\n",
       "         [ 4.3592e-02,  3.3271e-02],\n",
       "         [ 2.4700e-03,  2.4528e-03],\n",
       "         [-1.4878e-02, -6.3982e-05],\n",
       "         [ 2.5947e-03,  1.9774e-03],\n",
       "         [-1.5004e-02, -8.2328e-05],\n",
       "         [ 2.0036e-03,  1.9013e-03],\n",
       "         [-1.5472e-02,  6.5135e-04],\n",
       "         [-1.8116e-02,  3.8949e-03],\n",
       "         [-1.7010e-02,  5.6177e-03],\n",
       "         [-1.5738e-02,  6.5891e-03],\n",
       "         [-1.6275e-02,  2.6367e-03],\n",
       "         [-1.3733e-02,  4.6675e-03],\n",
       "         [-1.0841e-02,  7.6672e-03],\n",
       "         [-1.0211e-02,  9.7611e-03],\n",
       "         [-4.3335e-03,  1.8804e-02],\n",
       "         [ 1.2857e-03,  1.5034e-02],\n",
       "         [ 5.1988e-03,  9.3913e-03],\n",
       "         [ 1.4251e-03,  1.4884e-02],\n",
       "         [-4.3703e-03,  1.8709e-02],\n",
       "         [-4.0265e-03,  1.5827e-02],\n",
       "         [-4.7815e-03,  1.8990e-02],\n",
       "         [ 6.9434e-04,  1.5125e-02],\n",
       "         [-4.8701e-03,  1.8928e-02],\n",
       "         [-4.4681e-03,  1.4580e-02],\n",
       "         [-5.8146e-03,  2.3506e-02],\n",
       "         [-5.0353e-03,  1.5200e-02],\n",
       "         [-6.8366e-03,  2.4964e-02],\n",
       "         [-5.1788e-03,  1.6232e-02],\n",
       "         [-8.3879e-03,  2.5538e-02],\n",
       "         [ 2.6809e-03,  2.7304e-02],\n",
       "         [-8.8422e-03,  2.4328e-02],\n",
       "         [ 3.5768e-03,  2.6148e-02],\n",
       "         [ 1.9562e-02,  2.8575e-02],\n",
       "         [-4.2800e-03,  1.8626e-02],\n",
       "         [-4.6582e-03,  1.5165e-02],\n",
       "         [-4.6328e-03,  1.8887e-02],\n",
       "         [ 8.8069e-04,  1.4451e-02],\n",
       "         [-4.8134e-03,  1.8762e-02],\n",
       "         [-5.0242e-03,  1.4123e-02],\n",
       "         [-6.3939e-03,  2.2112e-02],\n",
       "         [-5.6014e-03,  1.4359e-02],\n",
       "         [-6.8369e-03,  2.3496e-02],\n",
       "         [-5.7549e-03,  1.5341e-02],\n",
       "         [-5.6215e-03,  1.7235e-02],\n",
       "         [-5.1035e-03,  1.6166e-02],\n",
       "         [-8.7471e-03,  2.4541e-02],\n",
       "         [ 2.3428e-03,  2.6193e-02],\n",
       "         [ 1.8113e-02,  2.8714e-02],\n",
       "         [ 3.3834e-02,  3.1180e-02],\n",
       "         [-6.2865e-03,  1.7844e-02],\n",
       "         [-1.5924e-03,  1.2564e-02],\n",
       "         [-6.0790e-03,  1.6701e-02],\n",
       "         [-5.3231e-03,  1.3806e-02],\n",
       "         [-5.9011e-03,  1.6464e-02],\n",
       "         [-5.2438e-03,  1.4576e-02],\n",
       "         [-5.7110e-03,  1.6074e-02],\n",
       "         [-5.1454e-03,  1.5518e-02],\n",
       "         [-5.1777e-03,  1.5337e-02],\n",
       "         [-2.5195e-03,  1.4618e-02],\n",
       "         [-3.5270e-03,  1.3575e-02],\n",
       "         [-5.3872e-03,  1.8692e-02],\n",
       "         [-7.3196e-03,  2.2241e-02],\n",
       "         [-5.8927e-03,  2.0124e-02],\n",
       "         [-1.8700e-03,  1.2916e-02],\n",
       "         [-5.5588e-03,  1.7874e-02],\n",
       "         [-5.2467e-03,  1.3892e-02],\n",
       "         [-6.8154e-03,  2.2343e-02],\n",
       "         [ 9.8192e-04,  2.8189e-02],\n",
       "         [-7.7643e-03,  2.3824e-02],\n",
       "         [-1.3292e-04,  2.7840e-02],\n",
       "         [ 1.5223e-02,  3.0022e-02],\n",
       "         [ 3.1171e-02,  3.2050e-02],\n",
       "         [ 4.7590e-02,  3.3642e-02],\n",
       "         [ 2.9734e-02,  3.1330e-02],\n",
       "         [-3.1003e-03,  1.8366e-02],\n",
       "         [-3.5751e-03,  1.5134e-02],\n",
       "         [-4.8301e-03,  2.0814e-02],\n",
       "         [-4.5068e-03,  1.4224e-02],\n",
       "         [-3.9470e-03,  1.9062e-02],\n",
       "         [ 1.3374e-03,  1.4109e-02],\n",
       "         [-3.9330e-03,  1.8180e-02],\n",
       "         [-4.0600e-03,  1.4312e-02],\n",
       "         [-5.8985e-03,  2.3522e-02],\n",
       "         [-4.2044e-03,  1.5347e-02],\n",
       "         [-3.6180e-03,  1.6400e-02],\n",
       "         [-1.2377e-04,  1.4476e-02],\n",
       "         [ 1.8820e-03,  6.3393e-03],\n",
       "         [ 5.7324e-04, -3.3390e-03],\n",
       "         [-1.4418e-02, -7.6978e-03],\n",
       "         [ 3.5297e-03, -3.5548e-03],\n",
       "         [ 4.6497e-03,  7.7257e-03],\n",
       "         [ 3.8040e-03,  1.5147e-02],\n",
       "         [ 5.3701e-03,  8.5204e-03],\n",
       "         [ 6.1691e-03, -1.3773e-03],\n",
       "         [ 6.5482e-03,  1.0328e-02],\n",
       "         [ 6.6003e-03,  1.5793e-02],\n",
       "         [ 3.6463e-03,  1.5470e-02],\n",
       "         [-1.1675e-05,  2.7944e-02],\n",
       "         [ 3.8978e-03,  1.9849e-02],\n",
       "         [-8.9021e-04,  2.9298e-02],\n",
       "         [ 5.0642e-03,  2.5271e-02],\n",
       "         [-3.7365e-03,  1.7844e-02],\n",
       "         [-3.0629e-03,  1.6551e-02],\n",
       "         [-4.8465e-03,  2.2711e-02],\n",
       "         [-4.8543e-03,  1.6095e-02],\n",
       "         [-4.7625e-03,  1.8816e-02],\n",
       "         [-5.7103e-03,  1.5369e-02],\n",
       "         [-6.7324e-03,  2.0977e-02],\n",
       "         [-6.5332e-03,  1.4336e-02],\n",
       "         [-7.1330e-03,  2.1735e-02],\n",
       "         [-1.3049e-03,  2.8715e-02],\n",
       "         [-7.6908e-03,  2.3290e-02],\n",
       "         [-7.3969e-03,  1.5417e-02],\n",
       "         [-7.1210e-03,  1.7852e-02],\n",
       "         [-6.4516e-03,  1.6365e-02],\n",
       "         [-6.0302e-03,  1.6611e-02],\n",
       "         [-3.1187e-03,  1.5002e-02],\n",
       "         [-2.9376e-04,  6.9007e-03],\n",
       "         [ 1.0895e-04, -1.4132e-03],\n",
       "         [-8.8792e-03, -1.0182e-02],\n",
       "         [-2.1614e-02, -1.0781e-02],\n",
       "         [-5.1578e-03, -1.2911e-02],\n",
       "         [-1.9467e-02, -1.4724e-02],\n",
       "         [-2.1788e-03, -1.1744e-02],\n",
       "         [ 6.0249e-03,  2.4475e-03],\n",
       "         [ 7.7469e-04, -8.8009e-03],\n",
       "         [-1.3350e-02, -1.6138e-02],\n",
       "         [ 3.2420e-03, -5.9064e-03],\n",
       "         [-1.1161e-02, -1.6730e-02],\n",
       "         [ 5.5196e-03, -3.3214e-03],\n",
       "         [ 1.1506e-02,  9.9274e-03],\n",
       "         [ 7.4729e-03, -3.0063e-04],\n",
       "         [ 1.2660e-02,  1.1100e-02],\n",
       "         [ 9.0147e-03,  1.9375e-03],\n",
       "         [-2.0886e-03, -1.1020e-02],\n",
       "         [ 1.1021e-02,  4.0963e-03],\n",
       "         [ 1.5135e-02,  1.3638e-02],\n",
       "         [ 1.3096e-02,  6.2246e-03],\n",
       "         [ 4.1738e-03, -2.1336e-03],\n",
       "         [ 1.5361e-02,  8.1436e-03],\n",
       "         [ 1.7760e-02,  1.7059e-02],\n",
       "         [ 1.7691e-02,  1.0698e-02],\n",
       "         [ 1.1361e-02,  4.8276e-03],\n",
       "         [ 2.0568e-02,  1.2907e-02],\n",
       "         [ 1.5282e-02,  7.8392e-03],\n",
       "         [ 1.9343e-03, -1.1889e-03],\n",
       "         [ 1.8560e-02,  1.1104e-02],\n",
       "         [ 6.0592e-03,  2.8408e-03],\n",
       "         [ 2.3802e-02,  1.4564e-02],\n",
       "         [ 2.8462e-02,  2.0712e-02],\n",
       "         [ 2.7260e-02,  1.8349e-02],\n",
       "         [ 1.8302e-02,  1.1449e-02],\n",
       "         [ 3.2869e-03,  6.7017e-03],\n",
       "         [ 2.5727e-02,  1.7067e-02],\n",
       "         [ 1.1846e-02,  1.1892e-02],\n",
       "         [ 4.1966e-03,  1.3705e-02],\n",
       "         [ 2.0346e-02,  1.6552e-02],\n",
       "         [ 3.7162e-02,  2.4561e-02],\n",
       "         [ 4.3683e-02,  3.0381e-02],\n",
       "         [ 4.5750e-02,  3.2017e-02],\n",
       "         [ 4.7776e-02,  4.4362e-02],\n",
       "         [ 4.9785e-02,  3.6245e-02],\n",
       "         [ 2.0060e-03,  6.0354e-03],\n",
       "         [-1.4729e-02,  5.4772e-03],\n",
       "         [-1.8245e-02,  8.0312e-03],\n",
       "         [-1.4441e-02,  6.1919e-03],\n",
       "         [-1.6570e-02,  8.0666e-03],\n",
       "         [-1.3603e-02,  6.8779e-03],\n",
       "         [-1.4665e-02,  9.4183e-03],\n",
       "         [-1.2996e-02,  1.0472e-02],\n",
       "         [-1.2003e-02,  1.0775e-02],\n",
       "         [-1.3292e-02,  1.1952e-02],\n",
       "         [-1.1628e-02,  1.2277e-02],\n",
       "         [-1.5535e-02,  1.5219e-02],\n",
       "         [-1.7227e-02,  1.8644e-02],\n",
       "         [-3.7747e-03,  1.9293e-02],\n",
       "         [-3.2687e-03,  1.8316e-02],\n",
       "         [-4.4221e-03,  1.9457e-02],\n",
       "         [-4.3254e-03,  1.7701e-02],\n",
       "         [-5.4318e-03,  2.4017e-02],\n",
       "         [-5.8748e-03,  1.6300e-02],\n",
       "         [-5.8211e-03,  1.9728e-02],\n",
       "         [-6.1308e-03,  1.5280e-02],\n",
       "         [-6.1793e-03,  1.9268e-02],\n",
       "         [-1.9242e-03,  1.4853e-02],\n",
       "         [ 2.2551e-03,  9.2866e-03],\n",
       "         [-1.9071e-03,  1.4818e-02],\n",
       "         [ 1.9622e-03,  9.0328e-03],\n",
       "         [-3.6513e-03, -7.3570e-04],\n",
       "         [ 2.4141e-03,  8.9310e-03],\n",
       "         [-1.0451e-03,  1.4620e-02],\n",
       "         [ 2.1814e-03,  8.5901e-03],\n",
       "         [-4.3445e-04, -1.1859e-03],\n",
       "         [ 2.6278e-03,  8.5831e-03],\n",
       "         [-6.5580e-05,  1.4812e-02],\n",
       "         [-2.8839e-03,  1.2833e-02],\n",
       "         [-2.7321e-05,  1.5636e-02],\n",
       "         [ 2.5138e-03,  8.1733e-03],\n",
       "         [ 5.4232e-03, -1.8023e-03],\n",
       "         [-1.1293e-02, -9.4368e-03],\n",
       "         [ 6.9374e-03, -1.4984e-03],\n",
       "         [ 4.6162e-03,  8.6861e-03],\n",
       "         [ 7.3534e-03, -9.7797e-04],\n",
       "         [-6.8771e-03, -1.3096e-02],\n",
       "         [-1.8807e-02, -8.1486e-03],\n",
       "         [-5.8653e-03, -1.3950e-02],\n",
       "         [ 1.0088e-02, -4.6510e-04],\n",
       "         [ 7.6559e-03,  9.6025e-03],\n",
       "         [ 3.9211e-03,  1.2004e-02],\n",
       "         [ 4.0694e-03,  1.2629e-02],\n",
       "         [ 4.5076e-03,  1.2629e-02],\n",
       "         [ 8.1101e-03,  1.2403e-02],\n",
       "         [ 8.7392e-03,  3.3760e-03],\n",
       "         [ 1.8290e-03, -1.0645e-02],\n",
       "         [ 1.0057e-02,  5.3930e-03],\n",
       "         [ 9.8176e-03,  1.5135e-02],\n",
       "         [ 8.1166e-03,  1.3137e-02],\n",
       "         [ 1.0565e-02,  1.6368e-02],\n",
       "         [ 1.2194e-02,  9.8493e-03],\n",
       "         [ 7.8357e-03, -1.0585e-03],\n",
       "         [-5.4570e-03, -1.4275e-02],\n",
       "         [ 1.0131e-02,  2.2285e-03],\n",
       "         [ 1.4573e-02,  1.2271e-02],\n",
       "         [ 1.4586e-02,  1.8293e-02],\n",
       "         [ 1.4573e-02,  2.0386e-02],\n",
       "         [ 1.5611e-02,  2.0272e-02],\n",
       "         [ 1.5179e-02,  2.4698e-02],\n",
       "         [ 1.7433e-02,  2.2685e-02],\n",
       "         [ 1.6694e-02,  2.9422e-02],\n",
       "         [ 1.1548e-02,  4.0619e-02],\n",
       "         [ 1.5085e-02,  4.3327e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_policy= Policy(dim_states, dim_actions,continuous_control)\n",
    "logits=RN_policy(sampled_obs)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7037, -0.7014, -0.7060, -0.7065, -0.6797, -0.7073, -0.7010, -0.6946,\n",
       "        -0.6872, -0.6799, -0.6983, -0.6932, -0.6858, -0.6928, -0.6857, -0.6931,\n",
       "        -0.6851, -0.6822, -0.6819, -0.6820, -0.6837, -0.6840, -0.6839, -0.7032,\n",
       "        -0.7048, -0.6863, -0.6911, -0.6999, -0.7048, -0.7031, -0.6813, -0.6860,\n",
       "        -0.7051, -0.7027, -0.7079, -0.6831, -0.7092, -0.6825, -0.7103, -0.7055,\n",
       "        -0.6767, -0.7045, -0.6977, -0.7047, -0.7031, -0.6815, -0.6864, -0.7050,\n",
       "        -0.7028, -0.7075, -0.6832, -0.7084, -0.6827, -0.6818, -0.7038, -0.7099,\n",
       "        -0.7051, -0.6985, -0.6918, -0.7053, -0.6861, -0.7046, -0.7028, -0.6820,\n",
       "        -0.7031, -0.6823, -0.7035, -0.6829, -0.6846, -0.7017, -0.7053, -0.7080,\n",
       "        -0.6802, -0.6858, -0.7049, -0.7028, -0.7078, -0.7068, -0.6775, -0.7072,\n",
       "        -0.7006, -0.6936, -0.6862, -0.6923, -0.7039, -0.7025, -0.7061, -0.6838,\n",
       "        -0.6817, -0.6868, -0.7043, -0.7024, -0.7080, -0.6834, -0.6832, -0.6859,\n",
       "        -0.6909, -0.6951, -0.6898, -0.6896, -0.6947, -0.6988, -0.6916, -0.6969,\n",
       "        -0.6950, -0.6978, -0.6991, -0.7072, -0.6852, -0.7084, -0.6831, -0.7040,\n",
       "        -0.7030, -0.7070, -0.6827, -0.6814, -0.7037, -0.7071, -0.6828, -0.7077,\n",
       "        -0.7083, -0.6778, -0.6818, -0.6807, -0.7046, -0.6819, -0.6841, -0.6896,\n",
       "        -0.6939, -0.6938, -0.6877, -0.6893, -0.6908, -0.6884, -0.6914, -0.6979,\n",
       "        -0.6945, -0.6886, -0.6959, -0.6887, -0.6924, -0.6970, -0.6924, -0.6967,\n",
       "        -0.6976, -0.6897, -0.6924, -0.6966, -0.6963, -0.6895, -0.6928, -0.6967,\n",
       "        -0.6964, -0.6893, -0.6969, -0.6947, -0.6894, -0.6948, -0.6885, -0.6893,\n",
       "        -0.6976, -0.6966, -0.6914, -0.6888, -0.6931, -0.6884, -0.6913, -0.6869,\n",
       "        -0.6865, -0.6863, -0.6914, -0.6999, -0.6911, -0.6831, -0.6801, -0.7035,\n",
       "        -0.6809, -0.7034, -0.6812, -0.6815, -0.7046, -0.6806, -0.7052, -0.6779,\n",
       "        -0.6754, -0.7047, -0.7040, -0.6813, -0.7042, -0.7080, -0.6821, -0.6805,\n",
       "        -0.7039, -0.6805, -0.6848, -0.6896, -0.7015, -0.6896, -0.6917, -0.6964,\n",
       "        -0.7010, -0.6899, -0.6935, -0.6961, -0.7006, -0.7010, -0.6853, -0.6903,\n",
       "        -0.6968, -0.6922, -0.6889, -0.6952, -0.6973, -0.6963, -0.6878, -0.6891,\n",
       "        -0.6879, -0.6941, -0.6972, -0.6974, -0.6891, -0.6910, -0.6958, -0.6994,\n",
       "        -0.6908, -0.6958, -0.6957, -0.6903, -0.6943, -0.6976, -0.6976, -0.6892,\n",
       "        -0.6920, -0.6950, -0.6961, -0.6908, -0.6979, -0.6905, -0.6995, -0.7078,\n",
       "        -0.7074], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribución de probabilidad categorica\n",
    "dist = torch.distributions.Categorical(logits=logits)\n",
    "log_probs=dist.log_prob(torch.tensor(sampled_action)).squeeze(0)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.4662, 10.4662, 10.4662, 10.4662, 10.4662, 10.4662, 10.4662, 10.4662,\n",
       "        10.4662, 10.4662, 10.4662, 12.2479, 12.2479, 12.2479, 12.2479, 12.2479,\n",
       "        12.2479, 12.2479, 12.2479, 12.2479, 12.2479, 12.2479, 12.2479, 12.2479,\n",
       "        17.3831, 17.3831, 17.3831, 17.3831, 17.3831, 17.3831, 17.3831, 17.3831,\n",
       "        17.3831, 17.3831, 17.3831, 17.3831, 17.3831, 17.3831, 17.3831, 17.3831,\n",
       "        17.3831, 17.3831, 17.3831, 14.8542, 14.8542, 14.8542, 14.8542, 14.8542,\n",
       "        14.8542, 14.8542, 14.8542, 14.8542, 14.8542, 14.8542, 14.8542, 14.8542,\n",
       "        14.8542, 14.8542, 14.8542, 13.9942, 13.9942, 13.9942, 13.9942, 13.9942,\n",
       "        13.9942, 13.9942, 13.9942, 13.9942, 13.9942, 13.9942, 13.9942, 13.9942,\n",
       "        13.9942, 13.9942,  9.5618,  9.5618,  9.5618,  9.5618,  9.5618,  9.5618,\n",
       "         9.5618,  9.5618,  9.5618,  9.5618, 23.7657, 23.7657, 23.7657, 23.7657,\n",
       "        23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 23.7657,\n",
       "        23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 23.7657,\n",
       "        23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 23.7657, 45.8315,\n",
       "        45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315,\n",
       "        45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315,\n",
       "        45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315,\n",
       "        45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315,\n",
       "        45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315,\n",
       "        45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315,\n",
       "        45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315, 45.8315,\n",
       "        45.8315, 45.8315, 45.8315, 45.8315, 12.2479, 12.2479, 12.2479, 12.2479,\n",
       "        12.2479, 12.2479, 12.2479, 12.2479, 12.2479, 12.2479, 12.2479, 12.2479,\n",
       "        12.2479, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399,\n",
       "        43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399,\n",
       "        43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399,\n",
       "        43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399,\n",
       "        43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399,\n",
       "        43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399,\n",
       "        43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399, 43.0399,\n",
       "        43.0399])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retorno_descontado=torch.tensor(estimate_returns(sampled_rew))\n",
    "retorno_descontado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-20.5965, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=torch.mean(log_probs*retorno_descontado)#.item()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=AdamW(RN_policy.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN_policy.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "#env=gym.make('Pendulum-v1')\n",
    "env.reset()\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 200\n",
      "Largo del episodio 200\n"
     ]
    }
   ],
   "source": [
    "# Sample rollouts (2 episodios): Ejecutar hasta que se generen solo 2 episodios!!\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 3)\n",
      "(400,)\n",
      "(241,)\n"
     ]
    }
   ],
   "source": [
    "sampled_obs = np.concatenate([x2[i][0] for i in range(len(x2))])\n",
    "sampled_action = np.concatenate([x2[i][1] for i in range(len(x2))])\n",
    "sampled_rew = [x2[i][2] for i in range(len(x2))]\n",
    "print(sampled_obs.shape)\n",
    "print(sampled_action.shape)\n",
    "print(sampled_reward.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_policy= Policy(dim_states, dim_actions,continuous_control)\n",
    "means=RN_policy(sampled_obs).squeeze(0).squeeze(1)\n",
    "means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_std=RN_policy.log_std\n",
    "std = torch.exp(log_std)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal(loc: torch.Size([1, 400]), scale: torch.Size([1, 400]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribución normal de parametros mean y std, esta se utiliza para muestrear acciones de modo de tal de explorar el espacio de acciones\n",
    "dist = torch.distributions.Normal(means, std)\n",
    "dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0811, -0.9321, -1.0659, -1.6953, -1.0631, -1.0431, -1.1569, -1.2348,\n",
       "        -0.9475, -1.0523, -0.9219, -0.9220, -0.9488, -0.9200, -0.9776, -0.9241,\n",
       "        -1.4021, -1.0944, -0.9541, -0.9260, -1.8441, -1.0149, -1.2902, -0.9190,\n",
       "        -1.4656, -0.9211, -6.0429, -1.4931, -0.9911, -1.8896, -1.3588, -1.4056,\n",
       "        -3.4738, -1.0023, -1.0110, -1.1330, -0.9292, -1.6947, -3.7863, -0.9945,\n",
       "        -1.7291, -1.7469, -1.0788, -1.0502, -1.1725, -1.0969, -1.3033, -0.9702,\n",
       "        -1.8855, -2.6720, -0.9902, -1.8468, -2.2699, -1.3912, -1.1131, -0.9285,\n",
       "        -1.0228, -1.7111, -3.8248, -2.6961, -1.0360, -1.2249, -1.4098, -0.9477,\n",
       "        -0.9930, -3.3119, -0.9556, -1.5082, -1.8430, -2.7212, -1.9516, -1.1005,\n",
       "        -1.0824, -2.7931, -1.5580, -1.5369, -1.9950, -0.9214, -0.9816, -1.2281,\n",
       "        -1.0832, -0.9190, -3.4521, -0.9592, -2.9980, -1.2907, -1.9814, -1.0013,\n",
       "        -1.0118, -2.0657, -0.9532, -0.9621, -4.6261, -4.4993, -1.7084, -1.7257,\n",
       "        -3.1592, -0.9244, -2.2328, -1.2500, -0.9190, -2.2364, -1.0031, -0.9943,\n",
       "        -1.4896, -0.9189, -0.9281, -0.9214, -2.1609, -1.7861, -2.1363, -2.0172,\n",
       "        -1.1468, -2.4149, -2.5401, -1.7393, -1.1635, -1.2445, -2.8086, -4.6613,\n",
       "        -2.4809, -0.9766, -1.7710, -1.0794, -0.9386, -2.8889, -1.8765, -1.3572,\n",
       "        -0.9270, -1.5410, -0.9190, -1.4422, -0.9376, -3.3660, -3.4314, -0.9463,\n",
       "        -0.9538, -1.0813, -1.0580, -1.3318, -1.1344, -1.2213, -0.9601, -1.3447,\n",
       "        -1.2087, -0.9640, -1.2292, -0.9682, -0.9824, -0.9879, -5.9084, -3.0361,\n",
       "        -0.9736, -2.0739, -5.0450, -1.4010, -1.0109, -1.1809, -0.9195, -0.9248,\n",
       "        -2.6824, -2.1394, -1.5982, -1.0035, -1.3564, -0.9265, -1.8365, -1.3600,\n",
       "        -1.0391, -2.2152, -1.7861, -2.2218, -1.5627, -1.0784, -1.3324, -1.2179,\n",
       "        -1.2546, -0.9947, -0.9202, -0.9346, -0.9429, -0.9413, -1.1401, -2.2395,\n",
       "        -1.1724, -2.0126, -0.9367, -0.9195, -2.1742, -0.9792, -1.5704, -1.3606,\n",
       "        -0.9773, -1.4072, -1.4759, -1.2916, -0.9330, -1.1835, -1.8066, -2.5601,\n",
       "        -7.7356, -0.9298, -1.6083, -1.0088, -0.9285, -0.9214, -1.3662, -0.9964,\n",
       "        -2.2667, -1.1434, -2.8594, -1.3354, -1.1029, -2.5686, -0.9272, -3.1477,\n",
       "        -1.9595, -0.9219, -0.9481, -2.3434, -1.5860, -1.0621, -2.2463, -4.0193,\n",
       "        -1.0745, -1.5893, -1.5251, -0.9209, -1.3763, -0.9191, -2.4492, -1.6005,\n",
       "        -0.9801, -1.4658, -0.9835, -1.9551, -0.9506, -0.9321, -0.9212, -2.8088,\n",
       "        -1.0906, -1.5190, -1.3668, -1.6830, -1.5561, -0.9653, -1.3279, -1.4946,\n",
       "        -1.1976, -0.9378, -0.9348, -0.9722, -1.0219, -0.9212, -1.0182, -1.1076,\n",
       "        -0.9478, -1.1628, -5.0755, -1.4201, -1.0147, -2.0209, -1.1457, -1.3574,\n",
       "        -1.0858, -1.3913, -0.9912, -0.9250, -1.1989, -1.0725, -0.9227, -0.9221,\n",
       "        -1.3157, -0.9910, -1.8839, -1.7378, -1.3882, -1.7066, -0.9339, -1.3114,\n",
       "        -1.8853, -0.9769, -1.1815, -2.0856, -2.0691, -1.2386, -1.4149, -1.3491,\n",
       "        -1.2523, -1.2150, -1.3238, -0.9528, -2.1864, -1.2030, -1.0245, -1.3608,\n",
       "        -1.2992, -2.2141, -1.2026, -1.1048, -1.0011, -1.2259, -0.9206, -1.1181,\n",
       "        -1.2798, -0.9364, -1.1009, -0.9467, -1.2718, -1.4563, -0.9707, -0.9327,\n",
       "        -1.2695, -0.9730, -1.2535, -1.5292, -1.0887, -2.2909, -2.7494, -1.1275,\n",
       "        -0.9603, -1.0342, -1.0544, -0.9275, -0.9328, -1.0039, -1.1712, -2.9889,\n",
       "        -0.9225, -0.9662, -1.8916, -1.8619, -1.0288, -2.0250, -2.7135, -0.9696,\n",
       "        -1.1072, -0.9563, -0.9829, -1.0638, -0.9375, -0.9485, -1.2634, -0.9469,\n",
       "        -1.9050, -0.9310, -1.1795, -1.1405, -1.9797, -1.5722, -1.4986, -2.6043,\n",
       "        -1.5330, -0.9211, -1.1895, -0.9220, -0.9385, -0.9339, -1.7684, -1.3094,\n",
       "        -2.0150, -0.9562, -1.3117, -1.2088, -1.2565, -1.6524, -1.8795, -2.7273,\n",
       "        -0.9371, -0.9466, -1.2803, -1.4251, -0.9345, -0.9973, -1.9489, -1.4446,\n",
       "        -2.6574, -2.1070, -1.1008, -1.0607, -1.5151, -1.0124, -0.9735, -3.3413,\n",
       "        -1.4114, -2.0286, -1.0065, -0.9763, -0.9985, -1.1520, -0.9947, -1.9839,\n",
       "        -1.0735, -1.2470, -2.7821, -1.2660, -1.0816, -3.0100, -0.9233, -1.2920],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs=dist.log_prob(torch.tensor((sampled_action))).squeeze(0)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -298.5952, -298.5952, -298.5952, -298.5952,\n",
       "        -298.5952, -298.5952, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143, -732.6143, -732.6143,\n",
       "        -732.6143, -732.6143, -732.6143, -732.6143])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retorno_descontado=torch.tensor(estimate_returns(sampled_rew))\n",
    "retorno_descontado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(763.8047, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=torch.mean(log_probs*retorno_descontado)#.item()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=AdamW(RN_policy.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN_policy.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Reducción de varianza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_returns( rollouts_rew):\n",
    "        estimated_returns = []\n",
    "        for rollout_rew in rollouts_rew:\n",
    "\n",
    "            # Largo del episodio (largo del reward)\n",
    "            n_steps = len(rollout_rew)\n",
    "            estimated_return = np.zeros(n_steps)\n",
    "\n",
    "            if _use_reward_to_go:\n",
    "                \n",
    "                vec_gammas=np.array([_gamma**j for j in range(n_steps)])\n",
    "\n",
    "                for t in range(n_steps):\n",
    "\n",
    "                    sum_descount=np.sum(vec_gammas[t:]*rollout_rew[t:])\n",
    "                    estimated_return[t] = sum_descount\n",
    "    \n",
    "            else:\n",
    "\n",
    "                estimated_return = np.zeros(n_steps)\n",
    "\n",
    "                vec_gammas=np.array([_gamma**j for j in range(n_steps)])\n",
    "\n",
    "                sum_descount=np.sum(vec_gammas*rollout_rew)\n",
    "\n",
    "                for t in range(n_steps):\n",
    "                    \n",
    "                    estimated_return[t] = sum_descount\n",
    "                     \n",
    "            estimated_returns = np.concatenate([estimated_returns, estimated_return])\n",
    "\n",
    "        if _use_baseline:\n",
    "            \n",
    "            average_return_baseline = np.mean(estimated_returns)\n",
    "            estimated_returns -= average_return_baseline\n",
    "\n",
    "        return np.array(estimated_returns, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largo del episodio 11\n",
      "Largo del episodio 13\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "continuous_control = isinstance(env.action_space, gym.spaces.Box)\n",
    "dim_actions = env.action_space.shape[0] if continuous_control else env.action_space.n\n",
    "\n",
    "policy_gradients_agent = PolicyGradients(dim_states=dim_states, \n",
    "                                             dim_actions=dim_actions, \n",
    "                                             lr=0.005,\n",
    "                                             gamma=0.99,\n",
    "                                             continuous_control=continuous_control,\n",
    "                                             reward_to_go=False,\n",
    "                                             use_baseline=False)\n",
    "\n",
    "# Sample rollouts (2 episodios): Ejecutar hasta que se generen solo 2 episodios!!\n",
    "x2=sample_rollouts(env, policy_gradients_agent, 1000, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 4)\n",
      "(24,)\n",
      "(241,)\n"
     ]
    }
   ],
   "source": [
    "sampled_obs = np.concatenate([x2[i][0] for i in range(len(x2))])\n",
    "sampled_action = np.concatenate([x2[i][1] for i in range(len(x2))])\n",
    "sampled_rew = [x2[i][2] for i in range(len(x2))]\n",
    "print(sampled_obs.shape)\n",
    "print(sampled_action.shape)\n",
    "print(sampled_reward.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.466174, 10.466174, 10.466174, 10.466174, 10.466174, 10.466174,\n",
       "       10.466174, 10.466174, 10.466174, 10.466174, 10.466174, 12.247898,\n",
       "       12.247898, 12.247898, 12.247898, 12.247898, 12.247898, 12.247898,\n",
       "       12.247898, 12.247898, 12.247898, 12.247898, 12.247898, 12.247898],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caso base\n",
    "_gamma=0.99\n",
    "_use_reward_to_go=False\n",
    "_use_baseline=False\n",
    "test=estimate_returns(sampled_rew)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.466174  ,  9.466174  ,  8.476174  ,  7.4960747 ,  6.5257754 ,\n",
       "        5.5651793 ,  4.6141896 ,  3.6727095 ,  2.740644  ,  1.8178993 ,\n",
       "        0.90438205, 12.247898  , 11.247898  , 10.257897  ,  9.277798  ,\n",
       "        8.307499  ,  7.346903  ,  6.3959126 ,  5.4544325 ,  4.522367  ,\n",
       "        3.5996225 ,  2.6861053 ,  1.7817231 ,  0.88638484], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reward to go\n",
    "_gamma=0.99\n",
    "_use_reward_to_go=True\n",
    "_use_baseline=False\n",
    "test=estimate_returns(sampled_rew)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.96510005, -0.96510005, -0.96510005, -0.96510005, -0.96510005,\n",
       "       -0.96510005, -0.96510005, -0.96510005, -0.96510005, -0.96510005,\n",
       "       -0.96510005,  0.8166231 ,  0.8166231 ,  0.8166231 ,  0.8166231 ,\n",
       "        0.8166231 ,  0.8166231 ,  0.8166231 ,  0.8166231 ,  0.8166231 ,\n",
       "        0.8166231 ,  0.8166231 ,  0.8166231 ,  0.8166231 ], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "_gamma=0.99\n",
    "_use_reward_to_go=False\n",
    "_use_baseline=True\n",
    "test=estimate_returns(sampled_rew)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.392932  ,  3.3929322 ,  2.4029322 ,  1.4228321 ,  0.45253316,\n",
       "       -0.50806284, -1.4590529 , -2.400533  , -3.3325984 , -4.255343  ,\n",
       "       -5.1688604 ,  6.1746554 ,  5.1746554 ,  4.184655  ,  3.2045553 ,\n",
       "        2.2342563 ,  1.2736603 ,  0.32267022, -0.61880994, -1.5508753 ,\n",
       "       -2.47362   , -3.3871372 , -4.291519  , -5.1868577 ], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline & reward-to-go\n",
    "_gamma=0.99\n",
    "_use_reward_to_go=True\n",
    "_use_baseline=True\n",
    "test=estimate_returns(sampled_rew)\n",
    "test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Evaluación del algoritmo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ti_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
