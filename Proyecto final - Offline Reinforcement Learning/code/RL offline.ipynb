{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Libreria de Reinforcement Learning Offline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import d3rlpy\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Stated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.datasets import get_cartpole # CartPole-v0 dataset\n",
    "from d3rlpy.datasets import get_pendulum # Pendulum-v0 dataset\n",
    "from d3rlpy.datasets import get_pybullet # PyBullet task datasets\n",
    "from d3rlpy.datasets import get_atari    # Atari 2600 task datasets\n",
    "from d3rlpy.datasets import get_d4rl     # D4RL datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# You can make your own dataset without any efforts. In this tutorial, let’s use integrated datasets to start. If you want to make a new dataset, see MDPDataset.\n",
    "dataset, env = get_cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can split dataset into a training dataset and a test dataset just like supervised learning as follows.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_episodes, test_episodes = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Algorithm\n",
    "# There are many algorithms avaiable in d3rlpy. Since CartPole is the simple task, let’s start from DQN, \n",
    "# which is the Q-learnig algorithm proposed as the first deep reinforcement learning algorithm.\n",
    "from d3rlpy.algos import DQN\n",
    "\n",
    "# if you don't use GPU, set use_gpu=False instead.\n",
    "dqn = DQN(use_gpu=False)\n",
    "\n",
    "# initialize neural networks with the given observation shape and action size.\n",
    "# this is not necessary when you directly call fit or fit_online method.\n",
    "dqn.build_with_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup metrics\n",
    "# Collecting evaluation metrics is important to train algorithms properly. In d3rlpy, the metrics is computed through scikit-learn style scorer functions.\n",
    "from d3rlpy.metrics.scorer import td_error_scorer\n",
    "from d3rlpy.metrics.scorer import average_value_estimation_scorer\n",
    "\n",
    "# calculate metrics with test dataset\n",
    "td_error = td_error_scorer(dqn, test_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since evaluating algorithms without access to environment is still difficult, \n",
    "# the algorithm can be directly evaluated with evaluate_on_environment function if the environment is available to interactF\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\n",
    "\n",
    "# set environment in scorer function\n",
    "evaluate_scorer = evaluate_on_environment(env)\n",
    "\n",
    "# evaluate algorithm on the environment\n",
    "rewards = evaluate_scorer(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:02.04 [debug    ] RoundIterator is selected.\n",
      "2023-07-11 11:02.04 [info     ] Directory is created at d3rlpy_logs\\DQN_20230711110204\n",
      "2023-07-11 11:02.04 [warning  ] Skip building models since they're already built.\n",
      "2023-07-11 11:02.04 [info     ] Parameters are saved to d3rlpy_logs\\DQN_20230711110204\\params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'bootstrap': False, 'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_reduction_type': 'min', 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 2457/2457 [00:10<00:00, 224.56it/s, loss=0.00986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:02.16 [info     ] DQN_20230711110204: epoch=1 step=2457 epoch=1 metrics={'time_sample_batch': 0.0001482949006542909, 'time_algorithm_update': 0.004112336419198297, 'loss': 0.009831182428302544, 'time_step': 0.004397009075973691, 'td_error': 0.9776492574275851, 'value_scale': 1.0429338020112338, 'environment': 45.4} step=2457\n",
      "2023-07-11 11:02.16 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_2457.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2457/2457 [00:15<00:00, 157.50it/s, loss=3.39e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:02.33 [info     ] DQN_20230711110204: epoch=2 step=4914 epoch=2 metrics={'time_sample_batch': 0.00017736653850774334, 'time_algorithm_update': 0.005923182973058233, 'loss': 3.3872496505164934e-05, 'time_step': 0.006269195771673418, 'td_error': 0.9793881064254958, 'value_scale': 1.0429349772492997, 'environment': 38.5} step=4914\n",
      "2023-07-11 11:02.33 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_4914.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 2457/2457 [00:10<00:00, 227.78it/s, loss=2.83e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:02.45 [info     ] DQN_20230711110204: epoch=3 step=7371 epoch=3 metrics={'time_sample_batch': 0.0001506175653304712, 'time_algorithm_update': 0.004053189107252904, 'loss': 2.821647643716045e-05, 'time_step': 0.0043414197998605806, 'td_error': 0.9800452881891559, 'value_scale': 1.0460872570608502, 'environment': 34.4} step=7371\n",
      "2023-07-11 11:02.45 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_7371.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 2457/2457 [00:10<00:00, 243.03it/s, loss=0.00577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:02.56 [info     ] DQN_20230711110204: epoch=4 step=9828 epoch=4 metrics={'time_sample_batch': 0.00014076205739947805, 'time_algorithm_update': 0.0037874211904039023, 'loss': 0.0057920699091158784, 'time_step': 0.004063795386265097, 'td_error': 0.9856932746186515, 'value_scale': 2.0086968765047555, 'environment': 9.2} step=9828\n",
      "2023-07-11 11:02.56 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_9828.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 2457/2457 [00:10<00:00, 225.60it/s, loss=0.00529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:03.08 [info     ] DQN_20230711110204: epoch=5 step=12285 epoch=5 metrics={'time_sample_batch': 0.00015394426624513904, 'time_algorithm_update': 0.004077951938192875, 'loss': 0.0052751559366512905, 'time_step': 0.004374477113219942, 'td_error': 0.9780053685763004, 'value_scale': 2.0306634705811204, 'environment': 14.3} step=12285\n",
      "2023-07-11 11:03.08 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_12285.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 2457/2457 [00:12<00:00, 193.28it/s, loss=0.00513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:03.22 [info     ] DQN_20230711110204: epoch=6 step=14742 epoch=6 metrics={'time_sample_batch': 0.00016445680803819842, 'time_algorithm_update': 0.004767206465271269, 'loss': 0.005133893604596525, 'time_step': 0.00510659400263015, 'td_error': 0.9714161495836863, 'value_scale': 2.028516243694792, 'environment': 14.7} step=14742\n",
      "2023-07-11 11:03.22 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_14742.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 2457/2457 [00:12<00:00, 201.64it/s, loss=0.0112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:03.35 [info     ] DQN_20230711110204: epoch=7 step=17199 epoch=7 metrics={'time_sample_batch': 0.00016617076300876044, 'time_algorithm_update': 0.004591330600842549, 'loss': 0.011203636822312181, 'time_step': 0.0048990725112436845, 'td_error': 0.9839518977196671, 'value_scale': 3.010638225111736, 'environment': 13.8} step=17199\n",
      "2023-07-11 11:03.35 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_17199.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 2457/2457 [00:12<00:00, 193.37it/s, loss=0.0143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:03.49 [info     ] DQN_20230711110204: epoch=8 step=19656 epoch=8 metrics={'time_sample_batch': 0.00016738488318040015, 'time_algorithm_update': 0.0047805579057009565, 'loss': 0.014307760829632614, 'time_step': 0.00509897159472393, 'td_error': 0.9794083182560483, 'value_scale': 2.995443596044703, 'environment': 22.9} step=19656\n",
      "2023-07-11 11:03.49 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_19656.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 2457/2457 [00:11<00:00, 206.48it/s, loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:04.03 [info     ] DQN_20230711110204: epoch=9 step=22113 epoch=9 metrics={'time_sample_batch': 0.00016558941758450425, 'time_algorithm_update': 0.00445583015134483, 'loss': 0.01409530609690859, 'time_step': 0.0047710417342661455, 'td_error': 0.9793419268516543, 'value_scale': 3.004581370889359, 'environment': 200.0} step=22113\n",
      "2023-07-11 11:04.03 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_22113.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 2457/2457 [00:12<00:00, 189.08it/s, loss=0.0175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:04.19 [info     ] DQN_20230711110204: epoch=10 step=24570 epoch=10 metrics={'time_sample_batch': 0.00017681828251591436, 'time_algorithm_update': 0.004890071409331816, 'loss': 0.01749501248355943, 'time_step': 0.00522161830378879, 'td_error': 1.008316008857332, 'value_scale': 3.965291892494718, 'environment': 200.0} step=24570\n",
      "2023-07-11 11:04.19 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711110204\\model_24570.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'time_sample_batch': 0.0001482949006542909,\n",
       "   'time_algorithm_update': 0.004112336419198297,\n",
       "   'loss': 0.009831182428302544,\n",
       "   'time_step': 0.004397009075973691,\n",
       "   'td_error': 0.9776492574275851,\n",
       "   'value_scale': 1.0429338020112338,\n",
       "   'environment': 45.4}),\n",
       " (2,\n",
       "  {'time_sample_batch': 0.00017736653850774334,\n",
       "   'time_algorithm_update': 0.005923182973058233,\n",
       "   'loss': 3.3872496505164934e-05,\n",
       "   'time_step': 0.006269195771673418,\n",
       "   'td_error': 0.9793881064254958,\n",
       "   'value_scale': 1.0429349772492997,\n",
       "   'environment': 38.5}),\n",
       " (3,\n",
       "  {'time_sample_batch': 0.0001506175653304712,\n",
       "   'time_algorithm_update': 0.004053189107252904,\n",
       "   'loss': 2.821647643716045e-05,\n",
       "   'time_step': 0.0043414197998605806,\n",
       "   'td_error': 0.9800452881891559,\n",
       "   'value_scale': 1.0460872570608502,\n",
       "   'environment': 34.4}),\n",
       " (4,\n",
       "  {'time_sample_batch': 0.00014076205739947805,\n",
       "   'time_algorithm_update': 0.0037874211904039023,\n",
       "   'loss': 0.0057920699091158784,\n",
       "   'time_step': 0.004063795386265097,\n",
       "   'td_error': 0.9856932746186515,\n",
       "   'value_scale': 2.0086968765047555,\n",
       "   'environment': 9.2}),\n",
       " (5,\n",
       "  {'time_sample_batch': 0.00015394426624513904,\n",
       "   'time_algorithm_update': 0.004077951938192875,\n",
       "   'loss': 0.0052751559366512905,\n",
       "   'time_step': 0.004374477113219942,\n",
       "   'td_error': 0.9780053685763004,\n",
       "   'value_scale': 2.0306634705811204,\n",
       "   'environment': 14.3}),\n",
       " (6,\n",
       "  {'time_sample_batch': 0.00016445680803819842,\n",
       "   'time_algorithm_update': 0.004767206465271269,\n",
       "   'loss': 0.005133893604596525,\n",
       "   'time_step': 0.00510659400263015,\n",
       "   'td_error': 0.9714161495836863,\n",
       "   'value_scale': 2.028516243694792,\n",
       "   'environment': 14.7}),\n",
       " (7,\n",
       "  {'time_sample_batch': 0.00016617076300876044,\n",
       "   'time_algorithm_update': 0.004591330600842549,\n",
       "   'loss': 0.011203636822312181,\n",
       "   'time_step': 0.0048990725112436845,\n",
       "   'td_error': 0.9839518977196671,\n",
       "   'value_scale': 3.010638225111736,\n",
       "   'environment': 13.8}),\n",
       " (8,\n",
       "  {'time_sample_batch': 0.00016738488318040015,\n",
       "   'time_algorithm_update': 0.0047805579057009565,\n",
       "   'loss': 0.014307760829632614,\n",
       "   'time_step': 0.00509897159472393,\n",
       "   'td_error': 0.9794083182560483,\n",
       "   'value_scale': 2.995443596044703,\n",
       "   'environment': 22.9}),\n",
       " (9,\n",
       "  {'time_sample_batch': 0.00016558941758450425,\n",
       "   'time_algorithm_update': 0.00445583015134483,\n",
       "   'loss': 0.01409530609690859,\n",
       "   'time_step': 0.0047710417342661455,\n",
       "   'td_error': 0.9793419268516543,\n",
       "   'value_scale': 3.004581370889359,\n",
       "   'environment': 200.0}),\n",
       " (10,\n",
       "  {'time_sample_batch': 0.00017681828251591436,\n",
       "   'time_algorithm_update': 0.004890071409331816,\n",
       "   'loss': 0.01749501248355943,\n",
       "   'time_step': 0.00522161830378879,\n",
       "   'td_error': 1.008316008857332,\n",
       "   'value_scale': 3.965291892494718,\n",
       "   'environment': 200.0})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Training\n",
    "dqn.fit(train_episodes,\n",
    "        eval_episodes=test_episodes,\n",
    "        n_epochs=10,\n",
    "        scorers={\n",
    "            'td_error': td_error_scorer,\n",
    "            'value_scale': average_value_estimation_scorer,\n",
    "            'environment': evaluate_scorer\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the training is done, your algorithm is ready to make decisions.\n",
    "observation = env.reset()\n",
    "\n",
    "# return actions based on the greedy-policy\n",
    "action = dqn.predict([observation])[0]\n",
    "\n",
    "# estimate action-values\n",
    "value = dqn.predict_value([observation], [action])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load\n",
    "# d3rlpy provides several ways to save trained models.\n",
    "\n",
    "# save full parameters\n",
    "dqn.save_model('dqn.pt')\n",
    "\n",
    "# load full parameters\n",
    "dqn2 = DQN()\n",
    "dqn2.build_with_dataset(dataset)\n",
    "dqn2.load_model('dqn.pt')\n",
    "\n",
    "# save the greedy-policy as TorchScript\n",
    "dqn.save_policy('policy.pt')\n",
    "\n",
    "# save the greedy-policy as ONNX\n",
    "dqn.save_policy('policy.onnx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with MDPDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "dataset, _ = d3rlpy.datasets.get_dataset(\"cartpole-random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first episode\n",
    "episode = dataset.episodes[0]\n",
    "\n",
    "# access to episode data\n",
    "episode.observations\n",
    "episode.actions\n",
    "episode.rewards\n",
    "\n",
    "# first transition\n",
    "transition = episode.transitions[0]\n",
    "\n",
    "# access to tuple\n",
    "transition.observation\n",
    "transition.action\n",
    "transition.reward\n",
    "transition.next_observation\n",
    "\n",
    "# linked list structure\n",
    "next_transition = transition.next_transition\n",
    "assert transition is next_transition.prev_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:25.42 [debug    ] RandomIterator is selected.\n",
      "2023-07-11 11:25.42 [info     ] Directory is created at d3rlpy_logs\\DQN_20230711112542\n",
      "2023-07-11 11:25.42 [debug    ] Building models...\n",
      "2023-07-11 11:25.42 [debug    ] Models have been built.\n",
      "2023-07-11 11:25.42 [info     ] Parameters are saved to d3rlpy_logs\\DQN_20230711112542\\params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'bootstrap': False, 'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_reduction_type': 'min', 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 10000/10000 [00:56<00:00, 177.85it/s, loss=0.00598]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:26.38 [info     ] DQN_20230711112542: epoch=1 step=10000 epoch=1 metrics={'time_sample_batch': 0.0003293832302093506, 'time_algorithm_update': 0.004991362094879151, 'loss': 0.005985863227147752, 'time_step': 0.005548441505432129} step=10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:26.38 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711112542\\model_10000.pt\n",
      "2023-07-11 11:26.38 [debug    ] RandomIterator is selected.\n",
      "2023-07-11 11:26.38 [info     ] Directory is created at d3rlpy_logs\\DQN_20230711112638\n",
      "2023-07-11 11:26.38 [warning  ] Skip building models since they're already built.\n",
      "2023-07-11 11:26.38 [info     ] Parameters are saved to d3rlpy_logs\\DQN_20230711112638\\params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'bootstrap': False, 'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_reduction_type': 'min', 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 10000/10000 [01:02<00:00, 160.27it/s, loss=0.0173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:27.41 [info     ] DQN_20230711112638: epoch=1 step=10000 epoch=1 metrics={'time_sample_batch': 0.0003325154542922974, 'time_algorithm_update': 0.005597547674179077, 'loss': 0.017282132790329342, 'time_step': 0.006155061841011047} step=10000\n",
      "2023-07-11 11:27.41 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711112638\\model_10000.pt\n",
      "2023-07-11 11:27.41 [debug    ] RandomIterator is selected.\n",
      "2023-07-11 11:27.41 [info     ] Directory is created at d3rlpy_logs\\DQN_20230711112741\n",
      "2023-07-11 11:27.41 [warning  ] Skip building models since they're already built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:27.41 [info     ] Parameters are saved to d3rlpy_logs\\DQN_20230711112741\\params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'bootstrap': False, 'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_reduction_type': 'min', 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 10000/10000 [01:02<00:00, 160.47it/s, loss=0.0227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:28.43 [info     ] DQN_20230711112741: epoch=1 step=10000 epoch=1 metrics={'time_sample_batch': 0.00035825092792510986, 'time_algorithm_update': 0.0055570454120635986, 'loss': 0.022723227619935643, 'time_step': 0.006148022413253784} step=10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:28.43 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711112741\\model_10000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'time_sample_batch': 0.00035825092792510986,\n",
       "   'time_algorithm_update': 0.0055570454120635986,\n",
       "   'loss': 0.022723227619935643,\n",
       "   'time_step': 0.006148022413253784})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed MDPDataset to Algorithm\n",
    "dqn = d3rlpy.algos.DQN()\n",
    "\n",
    "# feed as MDPDataset\n",
    "dqn.fit(dataset, n_steps=10000)\n",
    "\n",
    "# feed as Episode\n",
    "dqn.fit(dataset.episodes, n_steps=10000)\n",
    "\n",
    "# feed as Transition\n",
    "transitions = []\n",
    "for episode in dataset.episodes:\n",
    "    transitions.extend(episode.transitions)\n",
    "dqn.fit(transitions, n_steps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:29.07 [debug    ] RandomIterator is selected.\n",
      "2023-07-11 11:29.07 [info     ] Directory is created at d3rlpy_logs\\DQN_20230711112907\n",
      "2023-07-11 11:29.07 [warning  ] Skip building models since they're already built.\n",
      "2023-07-11 11:29.07 [info     ] Parameters are saved to d3rlpy_logs\\DQN_20230711112907\\params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'bootstrap': False, 'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_reduction_type': 'min', 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 10000/10000 [01:00<00:00, 164.72it/s, loss=0.0256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:30.09 [info     ] DQN_20230711112907: epoch=1 step=10000 epoch=1 metrics={'time_sample_batch': 0.0003408435583114624, 'time_algorithm_update': 0.005417067193984985, 'loss': 0.025643449136131678, 'time_step': 0.00598792986869812, 'soft_opc': nan, 'initial_value': 4.949024677276611} step=10000\n",
      "2023-07-11 11:30.09 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711112907\\model_10000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'time_sample_batch': 0.0003408435583114624,\n",
       "   'time_algorithm_update': 0.005417067193984985,\n",
       "   'loss': 0.025643449136131678,\n",
       "   'time_step': 0.00598792986869812,\n",
       "   'soft_opc': nan,\n",
       "   'initial_value': 4.949024677276611})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use scikit-learn utility\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# episode-wise split\n",
    "train_episodes, test_episodes = train_test_split(dataset.episodes)\n",
    "\n",
    "# setup metrics\n",
    "metrics = {\n",
    "  \"soft_opc\": d3rlpy.metrics.scorer.soft_opc_scorer(return_threshold=180),\n",
    "  \"initial_value\": d3rlpy.metrics.scorer.initial_state_value_estimation_scorer,\n",
    "}\n",
    "\n",
    "# start training with episode-wise splits\n",
    "dqn.fit(\n",
    "    train_episodes,\n",
    "    n_steps=10000,\n",
    "    scorers=metrics,\n",
    "    eval_episodes=test_episodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix Datasets\n",
    "from d3rlpy.dataset import MDPDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also mix multiple datasets to train algorithms.\n",
    "replay_dataset, _ = d3rlpy.datasets.get_dataset(\"cartpole-replay\")\n",
    "\n",
    "# extends replay dataset with random dataset\n",
    "replay_dataset.extend(dataset)\n",
    "\n",
    "# you can also save it and load it later\n",
    "replay_dataset.dump(\"mixed_dataset.h5\")\n",
    "mixed_dataset = MDPDataset.load(\"mixed_dataset.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d3rlpy provides APIs to support data collection from environments. This feature is specifically useful if you want to build your own original datasets for research or practice purposes.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Environment\n",
    "d3rlpy supports environments with OpenAI Gym interface. In this tutorial, let’s use simple CartPole environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection with Random Policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 11:38.08 [debug    ] Building model...\n",
      "2023-07-11 11:38.08 [debug    ] Model has been built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:04<00:00, 23225.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# If you want to collect experiences with uniformly random policy, you can use RandomPolicy and DiscreteRandomPolicy. This procedure corresponds to random datasets in D4RL.\n",
    "import d3rlpy\n",
    "\n",
    "# setup algorithm\n",
    "random_policy = d3rlpy.algos.DiscreteRandomPolicy()\n",
    "\n",
    "# prepare experience replay buffer\n",
    "buffer = d3rlpy.online.buffers.ReplayBuffer(maxlen=100000, env=env)\n",
    "\n",
    "# start data collection\n",
    "random_policy.collect(env, buffer, n_steps=100000)\n",
    "\n",
    "# export as MDPDataset\n",
    "dataset = buffer.to_mdp_dataset()\n",
    "\n",
    "# save MDPDataset\n",
    "dataset.dump(\"random_policy_dataset.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection with Trained Policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to collect experiences with previously trained policy, you can still use the same set of APIs. This procedure corresponds to medium datasets in D4RL.\n",
    "# setup algorithm\n",
    "dqn = d3rlpy.algos.DQN()\n",
    "\n",
    "# initialize neural networks before loading parameters\n",
    "dqn.build_with_env(env)\n",
    "\n",
    "# load pretrained parameters\n",
    "dqn.load_model(\"dqn_model.pt\")\n",
    "\n",
    "# prepare experience replay buffer\n",
    "buffer = d3rlpy.online.buffers.ReplayBuffer(maxlen=100000, env=env)\n",
    "\n",
    "# start data collection\n",
    "dqn.collect(env, buffer, n_steps=100000)\n",
    "\n",
    "# export as MDPDataset\n",
    "dataset = buffer.to_mdp_dataset()\n",
    "\n",
    "# save MDPDataset\n",
    "dataset.dump(\"trained_policy_dataset.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection while Training Policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup algorithm\n",
    "dqn = d3rlpy.algos.DQN()\n",
    "\n",
    "# prepare experience replay buffer\n",
    "buffer = d3rlpy.online.buffers.ReplayBuffer(maxlen=100000, env=env)\n",
    "\n",
    "# prepare exploration strategy if necessary\n",
    "explorer = d3rlpy.online.explorers.ConstantEpsilonGreedy(0.3)\n",
    "\n",
    "# start data collection\n",
    "dqn.fit_online(env, buffer, n_steps=100000)\n",
    "\n",
    "# export as MDPDataset\n",
    "dataset = buffer.to_mdp_dataset()\n",
    "\n",
    "# save MDPDataset\n",
    "dataset.dump(\"replay_dataset.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Your Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data collection API is introduced in Data Collection. In this tutorial, you can learn how to build your dataset from logged data such as the user data collected in your web service.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Logged Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, you need to prepare your logged data. In this tutorial, let’s use randomly generated data. terminals represents the last step of episodes. If terminals[i] == 1.0, i-th step is the terminal state. Otherwise you need to set zeros for non-terminal states.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# vector observation\n",
    "# 1000 steps of observations with shape of (100,)\n",
    "observations = np.random.random((1000, 100))\n",
    "\n",
    "# 1000 steps of actions with shape of (4,)\n",
    "actions = np.random.random((1000, 4))\n",
    "\n",
    "# 1000 steps of rewards\n",
    "rewards = np.random.random(1000)\n",
    "\n",
    "# 1000 steps of terminal flags\n",
    "terminals = np.random.randint(2, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = d3rlpy.dataset.MDPDataset(\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    rewards=rewards,\n",
    "    terminals=terminals,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode-wise split\n",
    "train_episodes, test_episodes = train_test_split(dataset.episodes)\n",
    "\n",
    "# setup metrics\n",
    "metrics = {\n",
    "  \"soft_opc\": d3rlpy.metrics.scorer.soft_opc_scorer(return_threshold=180),\n",
    "  \"initial_value\": d3rlpy.metrics.scorer.initial_state_value_estimation_scorer,\n",
    "}\n",
    "\n",
    "dqn = d3rlpy.algos.CQL()\n",
    "\n",
    "# start training with episode-wise splits\n",
    "dqn.fit(\n",
    "    train_episodes,\n",
    "    n_steps=10000,\n",
    "    scorers=metrics,\n",
    "    eval_episodes=test_episodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 steps of observations with shape of (100,)\n",
    "observations = np.random.random((1000, 100))\n",
    "# 1000 steps of actions with shape of (4,)\n",
    "actions = np.random.random((1000, 4))\n",
    "# 1000 steps of rewards\n",
    "rewards = np.random.random(1000)\n",
    "# 1000 steps of terminal flags\n",
    "terminals = np.random.randint(2, size=1000)\n",
    "\n",
    "dataset = MDPDataset(observations, actions, rewards, terminals)\n",
    "\n",
    "# automatically splitted into d3rlpy.dataset.Episode objects\n",
    "dataset.episodes\n",
    "\n",
    "# each episode is also splitted into d3rlpy.dataset.Transition objects\n",
    "episode = dataset.episodes[0]\n",
    "episode[0].observation\n",
    "episode[0].action\n",
    "episode[0].reward\n",
    "episode[0].next_observation\n",
    "episode[0].terminal\n",
    "\n",
    "# d3rlpy.dataset.Transition object has pointers to previous and next\n",
    "# transitions like linked list.\n",
    "transition = episode[0]\n",
    "while transition.next_transition:\n",
    "    transition = transition.next_transition\n",
    "\n",
    "# save as HDF5\n",
    "dataset.dump('dataset.h5')\n",
    "\n",
    "# load from HDF5\n",
    "new_dataset = MDPDataset.load('dataset.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode-wise split\n",
    "train_episodes, test_episodes = train_test_split(dataset.episodes)\n",
    "\n",
    "# setup metrics\n",
    "metrics = {\n",
    "  \"soft_opc\": d3rlpy.metrics.scorer.soft_opc_scorer(return_threshold=180),\n",
    "  \"initial_value\": d3rlpy.metrics.scorer.initial_state_value_estimation_scorer,\n",
    "}\n",
    "\n",
    "dqn = d3rlpy.algos.CQL()\n",
    "\n",
    "# start training with episode-wise splits\n",
    "dqn.fit(\n",
    "    train_episodes,\n",
    "    n_steps=10000,\n",
    "    scorers=metrics,\n",
    "    eval_episodes=test_episodes,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:00.17 [debug    ] RandomIterator is selected.\n",
      "2023-07-11 15:00.17 [info     ] Directory is created at d3rlpy_logs\\DQN_20230711150017\n",
      "2023-07-11 15:00.17 [debug    ] Building models...\n",
      "2023-07-11 15:00.17 [debug    ] Models have been built.\n",
      "2023-07-11 15:00.17 [info     ] Parameters are saved to d3rlpy_logs\\DQN_20230711150017\\params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 6.25e-05, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'bootstrap': False, 'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_reduction_type': 'min', 'target_update_interval': 8000, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 10000/10000 [00:57<00:00, 174.70it/s, loss=0.00445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:01.15 [info     ] DQN_20230711150017: epoch=1 step=10000 epoch=1 metrics={'time_sample_batch': 0.0003295729637145996, 'time_algorithm_update': 0.005085583972930908, 'loss': 0.004443957178714391, 'time_step': 0.005646009087562561, 'environment': 11.5} step=10000\n",
      "2023-07-11 15:01.15 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_10000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 10000/10000 [01:03<00:00, 158.64it/s, loss=0.00974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:02.18 [info     ] DQN_20230711150017: epoch=2 step=20000 epoch=2 metrics={'time_sample_batch': 0.00034461705684661864, 'time_algorithm_update': 0.005644531631469727, 'loss': 0.00974919555754168, 'time_step': 0.006218902659416199, 'environment': 11.7} step=20000\n",
      "2023-07-11 15:02.18 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_20000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 10000/10000 [01:05<00:00, 152.43it/s, loss=0.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:03.25 [info     ] DQN_20230711150017: epoch=3 step=30000 epoch=3 metrics={'time_sample_batch': 0.00036180675029754637, 'time_algorithm_update': 0.005866950345039368, 'loss': 0.02059645506235247, 'time_step': 0.006469796752929687, 'environment': 200.0} step=30000\n",
      "2023-07-11 15:03.25 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_30000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 10000/10000 [01:14<00:00, 134.51it/s, loss=0.0299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:04.40 [info     ] DQN_20230711150017: epoch=4 step=40000 epoch=4 metrics={'time_sample_batch': 0.0003942628860473633, 'time_algorithm_update': 0.0066790269851684575, 'loss': 0.029912366072440636, 'time_step': 0.007328744006156921, 'environment': 200.0} step=40000\n",
      "2023-07-11 15:04.40 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_40000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 10000/10000 [00:50<00:00, 199.99it/s, loss=0.0408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:05.31 [info     ] DQN_20230711150017: epoch=5 step=50000 epoch=5 metrics={'time_sample_batch': 0.0002907989740371704, 'time_algorithm_update': 0.004437735104560852, 'loss': 0.04076685120333568, 'time_step': 0.004935858654975891, 'environment': 200.0} step=50000\n",
      "2023-07-11 15:05.31 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_50000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 10000/10000 [00:50<00:00, 196.38it/s, loss=0.0509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:06.23 [info     ] DQN_20230711150017: epoch=6 step=60000 epoch=6 metrics={'time_sample_batch': 0.00030595717430114745, 'time_algorithm_update': 0.004517669820785522, 'loss': 0.05093365594213537, 'time_step': 0.005028146553039551, 'environment': 200.0} step=60000\n",
      "2023-07-11 15:06.23 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_60000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 10000/10000 [00:54<00:00, 183.81it/s, loss=0.0589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:07.18 [info     ] DQN_20230711150017: epoch=7 step=70000 epoch=7 metrics={'time_sample_batch': 0.00030831203460693357, 'time_algorithm_update': 0.004849077177047729, 'loss': 0.05888998315929493, 'time_step': 0.005369064044952392, 'environment': 186.2} step=70000\n",
      "2023-07-11 15:07.18 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_70000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 10000/10000 [01:03<00:00, 157.10it/s, loss=0.0682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:08.23 [info     ] DQN_20230711150017: epoch=8 step=80000 epoch=8 metrics={'time_sample_batch': 0.00035675320625305177, 'time_algorithm_update': 0.005683566427230835, 'loss': 0.06820846518946345, 'time_step': 0.006278869700431824, 'environment': 133.3} step=80000\n",
      "2023-07-11 15:08.23 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_80000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 10000/10000 [01:11<00:00, 140.49it/s, loss=0.0764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:09.34 [info     ] DQN_20230711150017: epoch=9 step=90000 epoch=9 metrics={'time_sample_batch': 0.00034275228977203367, 'time_algorithm_update': 0.006470977687835693, 'loss': 0.07638739716591081, 'time_step': 0.0070374038696289064, 'environment': 181.6} step=90000\n",
      "2023-07-11 15:09.34 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_90000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10000/10000 [00:49<00:00, 200.22it/s, loss=0.0815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:10.25 [info     ] DQN_20230711150017: epoch=10 step=100000 epoch=10 metrics={'time_sample_batch': 0.0002918296098709106, 'time_algorithm_update': 0.004440448784828186, 'loss': 0.08148027238445939, 'time_step': 0.00493109176158905, 'environment': 113.6} step=100000\n",
      "2023-07-11 15:10.25 [info     ] Model parameters are saved to d3rlpy_logs\\DQN_20230711150017\\model_100000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'time_sample_batch': 0.0003295729637145996,\n",
       "   'time_algorithm_update': 0.005085583972930908,\n",
       "   'loss': 0.004443957178714391,\n",
       "   'time_step': 0.005646009087562561,\n",
       "   'environment': 11.5}),\n",
       " (2,\n",
       "  {'time_sample_batch': 0.00034461705684661864,\n",
       "   'time_algorithm_update': 0.005644531631469727,\n",
       "   'loss': 0.00974919555754168,\n",
       "   'time_step': 0.006218902659416199,\n",
       "   'environment': 11.7}),\n",
       " (3,\n",
       "  {'time_sample_batch': 0.00036180675029754637,\n",
       "   'time_algorithm_update': 0.005866950345039368,\n",
       "   'loss': 0.02059645506235247,\n",
       "   'time_step': 0.006469796752929687,\n",
       "   'environment': 200.0}),\n",
       " (4,\n",
       "  {'time_sample_batch': 0.0003942628860473633,\n",
       "   'time_algorithm_update': 0.0066790269851684575,\n",
       "   'loss': 0.029912366072440636,\n",
       "   'time_step': 0.007328744006156921,\n",
       "   'environment': 200.0}),\n",
       " (5,\n",
       "  {'time_sample_batch': 0.0002907989740371704,\n",
       "   'time_algorithm_update': 0.004437735104560852,\n",
       "   'loss': 0.04076685120333568,\n",
       "   'time_step': 0.004935858654975891,\n",
       "   'environment': 200.0}),\n",
       " (6,\n",
       "  {'time_sample_batch': 0.00030595717430114745,\n",
       "   'time_algorithm_update': 0.004517669820785522,\n",
       "   'loss': 0.05093365594213537,\n",
       "   'time_step': 0.005028146553039551,\n",
       "   'environment': 200.0}),\n",
       " (7,\n",
       "  {'time_sample_batch': 0.00030831203460693357,\n",
       "   'time_algorithm_update': 0.004849077177047729,\n",
       "   'loss': 0.05888998315929493,\n",
       "   'time_step': 0.005369064044952392,\n",
       "   'environment': 186.2}),\n",
       " (8,\n",
       "  {'time_sample_batch': 0.00035675320625305177,\n",
       "   'time_algorithm_update': 0.005683566427230835,\n",
       "   'loss': 0.06820846518946345,\n",
       "   'time_step': 0.006278869700431824,\n",
       "   'environment': 133.3}),\n",
       " (9,\n",
       "  {'time_sample_batch': 0.00034275228977203367,\n",
       "   'time_algorithm_update': 0.006470977687835693,\n",
       "   'loss': 0.07638739716591081,\n",
       "   'time_step': 0.0070374038696289064,\n",
       "   'environment': 181.6}),\n",
       " (10,\n",
       "  {'time_sample_batch': 0.0002918296098709106,\n",
       "   'time_algorithm_update': 0.004440448784828186,\n",
       "   'loss': 0.08148027238445939,\n",
       "   'time_step': 0.00493109176158905,\n",
       "   'environment': 113.6})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import d3rlpy\n",
    "\n",
    "# setup replay CartPole-v0 dataset and environment\n",
    "dataset, env = d3rlpy.datasets.get_dataset(\"cartpole-replay\")\n",
    "\n",
    "# setup algorithm\n",
    "dqn = d3rlpy.algos.DQN()\n",
    "\n",
    "# start offline training\n",
    "dqn.fit(\n",
    "   dataset,\n",
    "   eval_episodes=dataset.episodes,\n",
    "   n_steps=100000,\n",
    "   n_steps_per_epoch=10000,\n",
    "   scorers={\n",
    "       \"environment\": d3rlpy.metrics.evaluate_on_environment(env),\n",
    "   },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ti_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
