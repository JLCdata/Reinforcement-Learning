{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deep Q-Network (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_states, dim_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        # MLP, fully connected layers, ReLU activations, linear ouput activation\n",
    "        # dim_states -> 64 -> 64 -> dim_actions\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim_states, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, dim_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # tensor format\n",
    "        #input = torch.from_numpy(input).unsqueeze(dim=0).float()\n",
    "\n",
    "        q_values = self.layers(input)\n",
    "\n",
    "        return q_values\n",
    "\n",
    "class DeepQNetworkAgent:\n",
    "\n",
    "    def __init__(self, dim_states, dim_actions, lr, gamma, epsilon, nb_training_steps, replay_buffer_size, batch_size):\n",
    "        \n",
    "        self._learning_rate = lr\n",
    "        self._gamma = gamma\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        self._epsilon_min = 0\n",
    "        self._epsilon_decay = self._epsilon / (nb_training_steps / 2.)\n",
    "\n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(dim_states=self._dim_states,\n",
    "                                          dim_actions=self._dim_actions,\n",
    "                                          max_size=replay_buffer_size,\n",
    "                                          sample_size=batch_size)\n",
    "\n",
    "        # Complete\n",
    "        self._deep_qnetwork = DeepQNetwork(self._dim_states, self._dim_actions)\n",
    "        self._target_deepq_network = copy.deepcopy(self._deep_qnetwork).eval()\n",
    "\n",
    "        # Adam optimizer\n",
    "        self._optimizer = AdamW(self._deep_qnetwork.parameters(), lr=self._learning_rate)\n",
    "\n",
    "\n",
    "    def store_transition(self, s_t, a_t, r_t, s_t1, done_t):\n",
    "        self.replay_buffer.store_transition(s_t, a_t, r_t, s_t1, done_t)\n",
    "\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        self._target_deepq_network.load_state_dict(self._deep_qnetwork.state_dict())\n",
    "        \n",
    "\n",
    "    def select_action(self, observation, greedy=False):\n",
    "        \n",
    "           \n",
    "            if np.random.random() > self._epsilon or greedy:\n",
    "                # Select action greedily\n",
    "\n",
    "                # Action values\n",
    "                qa = self._target_deepq_network(observation)\n",
    "\n",
    "                # Action con mayor q-value\n",
    "                action=qa.argmax().item()\n",
    "        \n",
    "            else:\n",
    "                # ExploraciÃ³n\n",
    "                action=np.random.randint(2)\n",
    "\n",
    "            if not greedy and self._epsilon >= self._epsilon_min:\n",
    "                \n",
    "                # Implement epsilon linear decay\n",
    "                self._epsilon-=self._epsilon_decay \n",
    "                \n",
    "\n",
    "            return action\n",
    "\n",
    "    def update(self):\n",
    "        s_t, a_t, r_t, s_t1, done_t=self.replay_buffer.sample_transitions()\n",
    "\n",
    "        qsa_b = q_network(state_b).gather(1, action_b)\n",
    "                \n",
    "        next_qsa_b = target_q_network(next_state_b)\n",
    "        next_qsa_b = torch.max(next_qsa_b, dim=-1, keepdim=True)[0]\n",
    "        \n",
    "        target_b = reward_b + ~done_b * gamma * next_qsa_b\n",
    "        loss = F.mse_loss(qsa_b, target_b)\n",
    "        q_network.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# Actions are discrete\n",
    "dim_actions = np.array(env.action_space.n)\n",
    "\n",
    "# States are continuous\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "print(dim_states)\n",
    "print(dim_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepQNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_qnetwork = DeepQNetwork(dim_states, dim_actions)\n",
    "deep_qnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04055779,  0.00673627, -0.00092249, -0.01014175], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation=env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0406,  0.0067, -0.0009, -0.0101]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = torch.from_numpy(observation).unsqueeze(dim=0).float()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1=observation\n",
    "tensor2=observation\n",
    "full=torch.cat([tensor1, tensor2], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0161, -0.0858],\n",
       "        [ 0.0161, -0.0858]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qsa=deep_qnetwork(full)\n",
    "qsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action=np.random.randint(2)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0161, 0.0161], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qsa=qsa[:,action]\n",
    "qsa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, dim_states, dim_actions, max_size, sample_size):\n",
    "\n",
    "        assert sample_size < max_size, \"Sample size cannot be greater than buffer size\"\n",
    "        \n",
    "        self._buffer_idx     = 0\n",
    "        self._exps_stored    = 0\n",
    "        self._buffer_size    = max_size\n",
    "        self._sample_size    = sample_size\n",
    "\n",
    "        self._s_t_array      = np.zeros((max_size, dim_states))\n",
    "        self._a_t_array      = np.zeros((max_size))\n",
    "        self._r_t_array      = np.zeros((max_size,))\n",
    "        self._s_t1_array     = np.zeros((max_size, dim_states))\n",
    "        self._term_t_array   = np.zeros((max_size,))\n",
    "\n",
    "\n",
    "    def store_transition(self, s_t, a_t, r_t, s_t1, done_t):\n",
    "\n",
    "        # Add transition to replay buffer according to self._buffer_idx\n",
    "        self._s_t_array[self._buffer_idx]=s_t   \n",
    "        self._a_t_array[self._buffer_idx]=a_t  \n",
    "        self._r_t_array[self._buffer_idx]=r_t  \n",
    "        self._s_t1_array[self._buffer_idx]=s_t1 \n",
    "        self._term_t_array[self._buffer_idx]=done_t\n",
    "\n",
    "        # Update replay buffer index\n",
    "        # Aumento de indice y reinicio de indice si superamos capacidad\n",
    "        self._buffer_idx = (self._buffer_idx + 1) % self._buffer_size\n",
    "        self._exps_stored += 1\n",
    "\n",
    "    \n",
    "    def sample_transitions(self):\n",
    "        assert self._exps_stored + 1 > self._sample_size, \"Not enough samples have been stored to start sampling\"\n",
    "        \n",
    "        sample_idxs = np.random.choice(self._buffer_size, size=self._sample_size,replace=False)\n",
    "        \n",
    "        return (self._s_t_array[sample_idxs],\n",
    "                self._a_t_array[sample_idxs],\n",
    "                self._r_t_array[sample_idxs],\n",
    "                self._s_t1_array[sample_idxs],\n",
    "                self._term_t_array[sample_idxs])\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Ambiente: CartPole\n",
    "env = gym.make('CartPole-v1')\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# Actions are discrete\n",
    "dim_actions = np.array(env.action_space.n)\n",
    "\n",
    "# States are continuous\n",
    "dim_states = env.observation_space.shape[0]\n",
    "\n",
    "print(dim_states)\n",
    "print(dim_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InicializaciÃ³n de memory Buffer\n",
    "max_size=5\n",
    "sample_size=3\n",
    "memory=ReplayBuffer(dim_states, dim_actions, max_size, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00939226  0.21154283 -0.01240931 -0.28007838] 1.0 False\n",
      "[-0.0051614   0.40683958 -0.01801088 -0.5766492 ] 1.0 False\n",
      "[ 0.00297539  0.21197465 -0.02954386 -0.28969413] 1.0 False\n",
      "[ 0.00721488  0.01728617 -0.03533775 -0.00647347] 1.0 False\n",
      "[ 0.0075606   0.21289663 -0.03546721 -0.3100931 ] 1.0 False\n",
      "[ 0.01181854  0.01829748 -0.04166908 -0.02880314] 1.0 False\n"
     ]
    }
   ],
   "source": [
    "# SimulaciÃ³n de 6 transiciones\n",
    "s_t=env.reset()\n",
    "\n",
    "for i in range(6):\n",
    "   \n",
    "    a_t=np.random.randint(2)\n",
    "    s_t1, r_t, done_t, _ = env.step(a_t)\n",
    "    print(s_t1, r_t, done_t)\n",
    "\n",
    "    # Guardar\n",
    "    memory.store_transition(s_t, a_t, r_t, s_t1, done_t)\n",
    "\n",
    "    s_t = s_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01181854,  0.01829748, -0.04166908, -0.02880314],\n",
       "       [-0.0051614 ,  0.40683958, -0.01801088, -0.57664919],\n",
       "       [ 0.00297539,  0.21197465, -0.02954386, -0.28969413],\n",
       "       [ 0.00721488,  0.01728617, -0.03533775, -0.00647347],\n",
       "       [ 0.0075606 ,  0.21289663, -0.03546721, -0.3100931 ]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos almacenados: Conjunto de estados almacenados\n",
    "memory._s_t1_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se observa como el sexto elemento de la transiciÃ³n de estados, coincide con el primer elemento del conjunto de estados almacenados. Esto debido a que la data se va sobreescribiendo en la medida de que hay nuevos registros.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se guardan a los 5 elementos, los cuales son el max_size del buffer\n",
    "memory._s_t1_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00721488,  0.01728617, -0.03533775, -0.00647347],\n",
       "        [ 0.00297539,  0.21197465, -0.02954386, -0.28969413],\n",
       "        [-0.00939226,  0.21154283, -0.01240931, -0.28007838]]),\n",
       " array([1., 0., 1.]),\n",
       " array([1., 1., 1.]),\n",
       " array([[ 0.0075606 ,  0.21289663, -0.03546721, -0.3100931 ],\n",
       "        [ 0.00721488,  0.01728617, -0.03533775, -0.00647347],\n",
       "        [-0.0051614 ,  0.40683958, -0.01801088, -0.57664919]]),\n",
       " array([0., 0., 0.]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "memory.sample_transitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se observa el muestreo de 3 elementos para el conjunto de estados\n",
    "len(memory.sample_transitions()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Q-Network (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_states, dim_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        # MLP, fully connected layers, ReLU activations, linear ouput activation\n",
    "        # dim_states -> 64 -> 64 -> dim_actions\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim_states, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, dim_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # tensor format\n",
    "        input = torch.from_numpy(input).unsqueeze(dim=0).float()\n",
    "\n",
    "        q_values = self.layers(input)\n",
    "\n",
    "        return q_values\n",
    "\n",
    "class DeepQNetworkAgent:\n",
    "\n",
    "    def __init__(self, dim_states, dim_actions, lr, gamma, epsilon, nb_training_steps, replay_buffer_size, batch_size):\n",
    "        \n",
    "        self._learning_rate = lr\n",
    "        self._gamma = gamma\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        self._epsilon_min = 0\n",
    "        self._epsilon_decay = self._epsilon / (nb_training_steps / 2.)\n",
    "\n",
    "        self._dim_states = dim_states\n",
    "        self._dim_actions = dim_actions\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(dim_states=self._dim_states,\n",
    "                                          dim_actions=self._dim_actions,\n",
    "                                          max_size=replay_buffer_size,\n",
    "                                          sample_size=batch_size)\n",
    "\n",
    "        # Complete\n",
    "        self._deep_qnetwork = DeepQNetwork(self._dim_states, self._dim_actions)\n",
    "        self._target_deepq_network = copy.deepcopy(self._deep_qnetwork).eval()\n",
    "\n",
    "        # Adam optimizer\n",
    "        self._optimizer = AdamW(self._deep_qnetwork.parameters(), lr=self._learning_rate)\n",
    "\n",
    "\n",
    "    def store_transition(self, s_t, a_t, r_t, s_t1, done_t):\n",
    "        self.replay_buffer.store_transition(s_t, a_t, r_t, s_t1, done_t)\n",
    "\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        self._target_deepq_network.load_state_dict(self._deep_qnetwork.state_dict())\n",
    "        \n",
    "\n",
    "    def select_action(self, observation, greedy=False):\n",
    "        \n",
    "           \n",
    "            if np.random.random() > self._epsilon or greedy:\n",
    "                # Select action greedily\n",
    "\n",
    "                # Action values\n",
    "                qa = self._target_deepq_network(observation)\n",
    "\n",
    "                # Action con mayor q-value\n",
    "                action=qa.argmax().item()\n",
    "        \n",
    "            else:\n",
    "                # ExploraciÃ³n\n",
    "                action=np.random.randint(2)\n",
    "\n",
    "            if not greedy and self._epsilon >= self._epsilon_min:\n",
    "                \n",
    "                # Implement epsilon linear decay\n",
    "                self._epsilon-=self._epsilon_decay \n",
    "                \n",
    "\n",
    "            return action\n",
    "\n",
    "    def update(self):\n",
    "        s_t, a_t, r_t, s_t1, done_t=self.replay_buffer.sample_transitions()\n",
    "\n",
    "        qsa_b = q_network(state_b).gather(1, action_b)\n",
    "                \n",
    "        next_qsa_b = target_q_network(next_state_b)\n",
    "        next_qsa_b = torch.max(next_qsa_b, dim=-1, keepdim=True)[0]\n",
    "        \n",
    "        target_b = reward_b + ~done_b * gamma * next_qsa_b\n",
    "        loss = F.mse_loss(qsa_b, target_b)\n",
    "        q_network.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anglo_american",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
